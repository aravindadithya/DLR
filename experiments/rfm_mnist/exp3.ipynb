{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ae23a4c-e713-4039-9d80-b967a7353fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "parent_dir='C:\\\\Users\\\\garav\\\\AGOP\\\\DLR'\n",
    "model_dir= 'C:\\\\Users\\\\garav\\\\AGOP\\\\DLR\\\\trained_models\\\\MNIST\\\\model4\\\\nn_models\\\\'\n",
    "#parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0dbcae2-852a-4c16-a540-9c9c2dd292cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from utils import agop_conv as agc\n",
    "from utils import trainer as tr\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import rfm\n",
    "import numpy as np\n",
    "from trained_models.MNIST.model4 import trainer as t\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.linalg import norm\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from utils.groupy.gconv.pytorch_gconv.splitgconv2d import P4ConvZ2, P4ConvP4\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd80ae41-ffa3-4ae8-87ab-8452edb259d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9c92e10d-06bf-41fb-8580-c35bf7601fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "83afe4f3-6178-42d2-a0fe-cef3228c6760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "trainloader, valloader, testloader = t.get_loaders()\n",
    "net= t.get_untrained_net()\n",
    "init_net= deepcopy(net)\n",
    "import os\n",
    "if os.path.exists(model_dir+'mnist_gcnn_trained_nn.pth'):\n",
    "    checkpoint = torch.load(model_dir+'mnist_gcnn_trained_nn.pth', map_location=torch.device(device))\n",
    "    net.load_state_dict(checkpoint['state_dict'])  # Access the 'state_dict' within the loaded dictionary\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "\n",
    "#print(\"Train the network first\")\n",
    "#t.train_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a1ba576-4192-4bd2-8d6e-dc55bfcf50d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garav\\AGOP\\DLR\\trained_models\\MNIST\\model4\\model4.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "tr.visualize_predictions(net, testloader, range(10), device, num_images=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "61e5bf8b-2d12-4016-9bd3-3ebd89848d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Without the incoming socket you cannot receive events from the server or register event handlers to your Visdom client.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef main():\\n\\n    # Adjust to index conv layers in VGGs\\n    idxs = list(range(8))\\n\\n    fname = \\'csv_logs/test.csv\\'\\n    outf = open(fname, \\'w\\')\\n\\n    net = models.vgg11(weights=\"DEFAULT\")\\n    #init_net is used as a reference untrained network.\\n    init_net = models.vgg11(weights=None)\\n\\n    # Modules is unused.\\n    modules= list(net.children())[:-1]\\n    modules += [nn.Flatten(), list(net.children())[-1]]\\n\\n    # Set path to imagenet data\\n    path = None\\n\\n    trainloader, _ = dataset.get_imagenet(batch_size=2, path=path)\\n\\n    for idx in idxs:\\n        i_val, r_val = verify_NFA(net, init_net, trainloader, layer_idx=idx)\\n        print(\"Layer \" + str(idx+1) + \\',\\' + str(i_val) + \\',\\' + str(r_val), file=outf, flush=True)\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' This module does the following\n",
    "1. Scan the network for conv layers\n",
    "2. For each conv layer compute W^TW of eq 3\n",
    "3. For each conv layer compute the AGOP(AJOP in case of multiple outputs)\n",
    "4. For each conv layer print the pearson correlation between 2 and 3\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "#from functorch import jacrev, vmap\n",
    "from torch.func import jacrev\n",
    "from torch.nn.functional import pad\n",
    "#import dataset\n",
    "from numpy.linalg import eig\n",
    "from copy import deepcopy\n",
    "from torch.linalg import norm, svd\n",
    "from torchvision import models\n",
    "import visdom\n",
    "from torch.linalg import norm, eig\n",
    "\n",
    "\n",
    "SEED = 2323\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "vis = visdom.Visdom('http://127.0.0.1', use_incoming_socket=False)\n",
    "vis.close(env='main')\n",
    "\n",
    "def patchify(x, patch_size, stride_size, padding=None, pad_type='zeros'):\n",
    "    '''\n",
    "        Given an input image (n,c,h,w) generate (n,w_out,h_out,c,q,s) respecting stride,padding, \n",
    "        w_out is number of pathces along the width for the given stride after padding\n",
    "        h_out is number of pathces along the height for the given stride after padding\n",
    "        (q,s) is the kernel dimensions \n",
    "    '''\n",
    "    q1, q2 = patch_size\n",
    "    s1, s2 = stride_size\n",
    "    print(\"image shape\",x)\n",
    "    if padding is None:\n",
    "        pad_1 = (q1-1)//2\n",
    "        pad_2 = (q2-1)//2\n",
    "    else:\n",
    "        pad_1, pad_2 = padding\n",
    "\n",
    "    pad_dims = (pad_2, pad_2, pad_1, pad_1)\n",
    "    if pad_type == 'zeros':\n",
    "        x = pad(x, pad_dims)\n",
    "    elif pad_type == 'circular':\n",
    "        x = pad(x, pad_dims, 'circular')\n",
    "        \n",
    "    patches = x.unfold(2, q1, s1).unfold(3, q2, s2) #(n, c, h_out, w_out, q, s)\n",
    "    patches = patches.transpose(1, 3).transpose(1, 2) #(n,w_out,h_out,c,q,s) \n",
    "    return patches\n",
    "\n",
    "class PatchConvLayer(nn.Module):\n",
    "    def __init__(self, conv_layer):\n",
    "        super().__init__()\n",
    "        self.layer = conv_layer #(k,c,q,s)\n",
    "\n",
    "    def forward(self, patches):\n",
    "        out = torch.einsum('nwhcqr, kcqr -> nwhk', patches, self.layer.weight)\n",
    "        n, w, h, k = out.shape\n",
    "        out = out.transpose(1, 3).transpose(2, 3) #(n,k,w_out,h_out)\n",
    "        return out\n",
    "\n",
    "def get_jacobian(net, data, c_idx=0, chunk=100):\n",
    "    with torch.no_grad():\n",
    "        def single_net(x):\n",
    "            # x is (w_out,h_out,c,q,s)\n",
    "            return net(x.unsqueeze(0))[:,c_idx*chunk:(c_idx+1)*chunk].squeeze(0)\n",
    "        # Parallelize across the images.\n",
    "        return torch.vmap(jacrev(single_net))(data) #(n, chunk, w_out, h_out, c, q, s)\n",
    "\n",
    "def egop(model, z):\n",
    "    ajop = 0\n",
    "    c = 10\n",
    "    chunk_idxs = 1\n",
    "    #Chunking is done to compute jacobian as chunks. This saves memory\n",
    "    #TODO: chunk should be passed as argument\n",
    "    chunk = c // chunk_idxs\n",
    "    for i in range(chunk_idxs):\n",
    "        J = get_jacobian(model, z, c_idx=i, chunk=chunk)\n",
    "        n, c, w, h, _, _, _ = J.shape\n",
    "        J = J.transpose(1, 3).transpose(1, 2) #(n, w_out, h_out, chunk, c, q, s)\n",
    "        grads = J.reshape(n*w*h, c, -1) #(n*w_out*h_out, chunk, c*q*s)\n",
    "        #Clarify: Where is mean taken\n",
    "        ajop += torch.einsum('ncd, ncD -> dD', grads, grads) #(c*q*s,c*q*s)\n",
    "    return ajop\n",
    "\n",
    "\n",
    "def load_nn(net, init_net, layer_idx=0):\n",
    "    '#TODO: Replace net with net.features'\n",
    "    \n",
    "    count = 0\n",
    "    # Get the layer_idx+1 th conv layer\n",
    "    for idx, m in enumerate(net.children()):\n",
    "        if isinstance(m, P4ConvZ2):\n",
    "            count += 1\n",
    "        if count-1 == layer_idx:\n",
    "            l_idx = idx\n",
    "            break\n",
    "\n",
    "    layers = list(net.children())\n",
    "    layers_init= list(init_net.children())\n",
    "    layer= layers[l_idx]\n",
    "    layer_init = layers_init[l_idx]\n",
    "    \n",
    "    # Extract all the meta info of the current conv layer.\n",
    "    (q, s) = layer.kernel_size\n",
    "    (pad1, pad2) = layer.padding\n",
    "    (s1, s2) = layer.stride\n",
    "    \n",
    "    # Extract W matrix\n",
    "    M = layer.weight\n",
    "    M0= layer_init.weight\n",
    "    \n",
    "    # Truncate all layers before l_idx and wrap the current conv layer as a PatchConvLayer class. \n",
    "    patchnet = deepcopy(net)\n",
    "    patchnet = layers[l_idx:]\n",
    "    layer = PatchConvLayer(layers[l_idx])\n",
    "    patchnet[0] = layer\n",
    "    \n",
    "\n",
    "    k, ki, ip_stab, q,s= M.shape\n",
    "            \n",
    "    # Build W which is a (k, c*q*s) matrix. Check ip_stab usage\n",
    "    M = M.reshape(-1, ki*ip_stab*q*s)\n",
    "            \n",
    "    # Compute WtW which is (c*q*s,c*q*s) matrix\n",
    "    M = torch.einsum('nd, nD -> dD', M, M)\n",
    "            \n",
    "    # Build W0 from the untrained net which is a (k, c*q*s) matrix         \n",
    "    M0 = M0.reshape(-1, ki*ip_stab*q*s)\n",
    "    \n",
    "    # Compute W0tW0 which is (c*q*s,c*q*s) matrix\n",
    "    M0 = torch.einsum('nd, nD -> dD', M0, M0)\n",
    "\n",
    "    return net, patchnet, M, M0, l_idx, [(q, s), (pad1,pad2), (s1,s2)]\n",
    "\n",
    "\n",
    "def get_grads(net, patchnet, trainloader,\n",
    "              kernel=(3,3), padding=(1,1),\n",
    "              stride=(1,1), layer_idx=0):\n",
    "    net.eval()\n",
    "    net.cuda()\n",
    "    patchnet.eval()\n",
    "    patchnet.cuda()\n",
    "    M = 0\n",
    "    q, s = kernel\n",
    "    pad1, pad2 = padding\n",
    "    s1, s2 = stride\n",
    "\n",
    "    # Num images for taking AGOP (Can be small for early layers)\n",
    "    MAX_NUM_IMGS = 10\n",
    "\n",
    "    for idx, batch in enumerate(trainloader):\n",
    "        print(\"Computing GOP for sample \" + str(idx) + \\\n",
    "              \" out of \" + str(MAX_NUM_IMGS))\n",
    "        imgs, _ = batch\n",
    "        with torch.no_grad():\n",
    "            imgs = imgs.cuda()        \n",
    "            # Run the first half of the network wrt to the current layer \n",
    "            imgs = net.features[:layer_idx](imgs).cpu() #(n,c,h,w)\n",
    "        patches = patchify(imgs, (q, s), (s1,s2), padding=(pad1,pad2))#(n,w_out,h_out,c,q,s)\n",
    "        patches = patches.cuda()\n",
    "        #print(patches.shape)\n",
    "        M += egop(patchnet, patches).cpu()\n",
    "        del imgs, patches\n",
    "        torch.cuda.empty_cache()\n",
    "        if idx >= MAX_NUM_IMGS:\n",
    "            break\n",
    "    net.cpu()\n",
    "    patchnet.cpu()\n",
    "    return M\n",
    "\n",
    "\n",
    "def min_max(M):\n",
    "    return (M - M.min()) / (M.max() - M.min())\n",
    "\n",
    "\n",
    "def correlation(M1, M2):\n",
    "    M1 -= M1.mean()\n",
    "    M2 -= M2.mean()\n",
    "\n",
    "    norm1 = norm(M1.flatten())\n",
    "    norm2 = norm(M2.flatten())\n",
    "\n",
    "    return torch.sum(M1.cuda() * M2.cuda()) / (norm1 * norm2)\n",
    "\n",
    "\n",
    "def verify_NFA(net, init_net, trainloader, layer_idx=0):\n",
    "\n",
    "\n",
    "    net, patchnet, M, M0, l_idx, conv_vals = load_nn(net,\n",
    "                                                     init_net,\n",
    "                                                     layer_idx=layer_idx)\n",
    "    (q, s), (pad1, pad2), (s1, s2) = conv_vals\n",
    "\n",
    "    i_val = correlation(M0, M)\n",
    "    print(\"Correlation between Initial and Trained CNFM: \", i_val)\n",
    "\n",
    "    G = get_grads(net, patchnet, trainloader,\n",
    "                  kernel=(q, s),\n",
    "                  padding=(pad1, pad2),\n",
    "                  stride=(s1, s2),\n",
    "                  layer_idx=l_idx)\n",
    "    print(\"Shpae after gradients: \", G.shape)\n",
    "    G = sqrt(G)\n",
    "    Gop = G.clone()\n",
    "    r_val = correlation(M, G)\n",
    "    print(\"Correlation between Trained CNFM and AGOP: \", r_val)\n",
    "    print(\"Final: \", i_val, r_val)\n",
    "    return Gop \n",
    "    #return i_val.data.numpy(), r_val.data.numpy()\n",
    "\n",
    "\n",
    "\n",
    "def sqrt(G):\n",
    "    U, s, Vt = svd(G)\n",
    "    s = torch.pow(s, 1./2)\n",
    "    G = U @ torch.diag(s) @ Vt\n",
    "    return G\n",
    "\n",
    "'''\n",
    "def main():\n",
    "\n",
    "    # Adjust to index conv layers in VGGs\n",
    "    idxs = list(range(8))\n",
    "\n",
    "    fname = 'csv_logs/test.csv'\n",
    "    outf = open(fname, 'w')\n",
    "\n",
    "    net = models.vgg11(weights=\"DEFAULT\")\n",
    "    #init_net is used as a reference untrained network.\n",
    "    init_net = models.vgg11(weights=None)\n",
    "\n",
    "    # Modules is unused.\n",
    "    modules= list(net.children())[:-1]\n",
    "    modules += [nn.Flatten(), list(net.children())[-1]]\n",
    "\n",
    "    # Set path to imagenet data\n",
    "    path = None\n",
    "\n",
    "    trainloader, _ = dataset.get_imagenet(batch_size=2, path=path)\n",
    "\n",
    "    for idx in idxs:\n",
    "        i_val, r_val = verify_NFA(net, init_net, trainloader, layer_idx=idx)\n",
    "        print(\"Layer \" + str(idx+1) + ',' + str(i_val) + ',' + str(r_val), file=outf, flush=True)\n",
    "'''\n",
    "#TODO: ADD a visualizer for the image\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e1016c9c-9f91-4650-8e54-c3013b26ff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "0\n",
      "Parameter containing:\n",
      "tensor([[[[[ 0.0292,  0.2141,  0.0165],\n",
      "           [ 0.4765,  0.1134, -0.4281],\n",
      "           [ 0.2660, -0.1225,  0.1569]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.3971,  0.3040, -0.0782],\n",
      "           [-0.2174,  0.3545, -0.1848],\n",
      "           [ 0.1032,  0.2699,  0.3735]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.1157,  0.3254, -0.1751],\n",
      "           [-0.1815, -0.2930, -0.2022],\n",
      "           [ 0.1058, -0.1116, -0.3284]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.2150, -0.0393, -0.3234],\n",
      "           [ 0.4192, -0.2133, -0.1919],\n",
      "           [ 0.4808,  0.4288, -0.2992]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.0141,  0.2815, -0.0548],\n",
      "           [ 0.2867, -0.1359, -0.3368],\n",
      "           [ 0.0087, -0.2200,  0.1808]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.4491, -0.2679, -0.2949],\n",
      "           [ 0.0531,  0.1287, -0.3561],\n",
      "           [ 0.2307, -0.2356, -0.3471]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.0144,  0.2857,  0.1793],\n",
      "           [ 0.2572,  0.3503, -0.1913],\n",
      "           [ 0.1426, -0.2798, -0.1957]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.3386,  0.1300, -0.1025],\n",
      "           [ 0.1340,  0.2924, -0.4710],\n",
      "           [ 0.5221,  0.1273, -0.2904]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.0140, -0.2038, -0.2024],\n",
      "           [ 0.0614,  0.1979, -0.1601],\n",
      "           [-0.3146,  0.0530, -0.0572]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.3300,  0.1862, -0.0882],\n",
      "           [ 0.0076,  0.5200,  0.1462],\n",
      "           [ 0.0123,  0.3221,  0.1396]]]]], requires_grad=True)\n",
      "(3, 3)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "[PatchConvLayer(\n",
      "  (layer): P4ConvZ2()\n",
      "), P4ConvP4(), P4ConvP4(), P4ConvP4(), Linear(in_features=1280, out_features=50, bias=True), Linear(in_features=50, out_features=10, bias=True)]\n",
      "tensor([[ 0.6440, -0.0974, -0.2551,  0.2405,  0.1014, -0.2747,  0.3693,  0.0123,\n",
      "         -0.4480],\n",
      "        [-0.0974,  0.5714,  0.0618,  0.1066,  0.1669, -0.2615,  0.2195, -0.0104,\n",
      "          0.1399],\n",
      "        [-0.2551,  0.0618,  0.3231, -0.0910,  0.0108,  0.2619, -0.2116, -0.1631,\n",
      "          0.2140],\n",
      "        [ 0.2405,  0.1066, -0.0910,  0.6559,  0.0540, -0.4440,  0.3888, -0.0418,\n",
      "         -0.1305],\n",
      "        [ 0.1014,  0.1669,  0.0108,  0.0540,  0.8227, -0.1743,  0.1084,  0.1398,\n",
      "          0.1489],\n",
      "        [-0.2747, -0.2615,  0.2619, -0.4440, -0.1743,  0.8407, -0.5527,  0.1329,\n",
      "          0.2541],\n",
      "        [ 0.3693,  0.2195, -0.2116,  0.3888,  0.1084, -0.5527,  0.7691,  0.1472,\n",
      "         -0.3366],\n",
      "        [ 0.0123, -0.0104, -0.1631, -0.0418,  0.1398,  0.1329,  0.1472,  0.5891,\n",
      "          0.0917],\n",
      "        [-0.4480,  0.1399,  0.2140, -0.1305,  0.1489,  0.2541, -0.3366,  0.0917,\n",
      "          0.6601]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class PatchConvLayer(nn.Module):\n",
    "    def __init__(self, conv_layer):\n",
    "        super().__init__()\n",
    "        self.layer = conv_layer #(k,c,q,s)\n",
    "\n",
    "    def forward(self, patches):\n",
    "        out = torch.einsum('nwhcqr, kcqr -> nwhk', patches, self.layer.weight)\n",
    "        n, w, h, k = out.shape\n",
    "        out = out.transpose(1, 3).transpose(2, 3) #(n,k,w_out,h_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "layer_idx=0\n",
    "count=0\n",
    "# Get the layer_idx+1 th conv layer\n",
    "for idx, m in enumerate(net.children()):\n",
    "        if isinstance(m, P4ConvZ2):\n",
    "            print(\"hi\")\n",
    "            count += 1\n",
    "        if count-1 == layer_idx:\n",
    "            l_idx = idx\n",
    "            break\n",
    "layers= list(net.children())\n",
    "\n",
    "layer = layers[l_idx]\n",
    "print(l_idx)\n",
    "print(layer.weight)\n",
    "print(layer.kernel_size)\n",
    "print(layer.padding)\n",
    "print(layer.stride)\n",
    "# Extract W matrix\n",
    "M = layer.weight\n",
    "#M0= layer_init.weight\n",
    "patchnet = deepcopy(net)\n",
    "layer = PatchConvLayer(layers[l_idx])\n",
    "patchnet= layers[l_idx:]\n",
    "patchnet[0] = layer\n",
    "print(patchnet)\n",
    "\n",
    "#_, ki, q, s = M.shape\n",
    "#print(M.shape)\n",
    "#_,ki, q, s = M.shape\n",
    "\n",
    "k, ki, ip_stab, q,s= M.shape\n",
    "            \n",
    "# Build W which is a (k, c*q*s) matrix. What to do with ip_stab\n",
    "M = M.reshape(-1, ki*ip_stab*q*s)\n",
    "            \n",
    "# Compute WtW which is (c*q*s,c*q*s) matrix\n",
    "M = torch.einsum('nd, nD -> dD', M, M)\n",
    "\n",
    "M0=M\n",
    "# Build W0 from the untrained net which is a (k, c*q*s) matrix         \n",
    "M0 = M0.reshape(-1, ki*ip_stab*q*s)\n",
    "    \n",
    "# Compute W0tW0 which is (c*q*s,c*q*s) matrix\n",
    "M0 = torch.einsum('nd, nD -> dD', M0, M0)\n",
    "\n",
    "print(M)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b23ff-d3b5-4636-8e84-13f276a135ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "net.cuda()\n",
    "patchnet.eval()\n",
    "patchnet.cuda()\n",
    "M = 0\n",
    "q, s = (3,3)\n",
    "pad1, pad2 = (0,0)\n",
    "s1, s2 = (1,1)\n",
    "MAX_NUM_IMGS = 10\n",
    "for idx, batch in enumerate(trainloader):\n",
    "        print(\"Computing GOP for sample \" + str(idx) + \\\n",
    "              \" out of \" + str(MAX_NUM_IMGS))\n",
    "        imgs, _ = batch\n",
    "        with torch.no_grad():\n",
    "            imgs = imgs.cuda()        \n",
    "            # Run the first half of the network wrt to the current layer \n",
    "            imgs = net.children()[:layer_idx](imgs).cpu() #(n,c,h,w)\n",
    "        patches = patchify(imgs, (q, s), (s1,s2), padding=(pad1,pad2))#(n,w_out,h_out,c,q,s)\n",
    "        patches = patches.cuda()\n",
    "        #print(patches.shape)\n",
    "        M += egop(patchnet, patches).cpu()\n",
    "        del imgs, patches\n",
    "        torch.cuda.empty_cache()\n",
    "        if idx >= MAX_NUM_IMGS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "85682506-517b-4ce7-a079-f2d94fda4c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor Shape: torch.Size([2, 3, 5, 5])\n",
      "Rotated 4 Times Tensor Shape: torch.Size([2, 3, 5, 4, 5])\n",
      "\n",
      "Original slice (k=0, c=0):\n",
      " tensor([[ 1.9671,  1.9197,  0.4562,  2.3616, -1.6425],\n",
      "        [ 0.4313, -1.1668, -2.9183, -1.4771, -0.3668],\n",
      "        [ 1.2567,  0.7854, -1.8855,  0.5730, -1.4748],\n",
      "        [-0.5583,  1.5608, -0.2936,  0.5151,  0.3046],\n",
      "        [ 1.1858, -0.7286,  0.4793,  0.1773, -0.5086]])\n",
      "\n",
      "0-degree rotated slice (k=0, c=0, rotation=0):\n",
      " tensor([[ 1.9671,  1.9197,  0.4562,  2.3616, -1.6425],\n",
      "        [ 0.4313, -1.1668, -2.9183, -1.4771, -0.3668],\n",
      "        [ 1.2567,  0.7854, -1.8855,  0.5730, -1.4748],\n",
      "        [-0.5583,  1.5608, -0.2936,  0.5151,  0.3046],\n",
      "        [ 1.1858, -0.7286,  0.4793,  0.1773, -0.5086]])\n",
      "\n",
      "90-degree rotated slice (k=0, c=0, rotation=1):\n",
      " tensor([[ 1.1858, -0.7286,  0.4793,  0.1773, -0.5086],\n",
      "        [-0.5583,  1.5608, -0.2936,  0.5151,  0.3046],\n",
      "        [ 1.2567,  0.7854, -1.8855,  0.5730, -1.4748],\n",
      "        [ 0.4313, -1.1668, -2.9183, -1.4771, -0.3668],\n",
      "        [ 1.9671,  1.9197,  0.4562,  2.3616, -1.6425]])\n",
      "\n",
      "180-degree rotated slice (k=0, c=0, rotation=2):\n",
      " tensor([[-0.5086,  0.1773,  0.4793, -0.7286,  1.1858],\n",
      "        [ 0.3046,  0.5151, -0.2936,  1.5608, -0.5583],\n",
      "        [-1.4748,  0.5730, -1.8855,  0.7854,  1.2567],\n",
      "        [-0.3668, -1.4771, -2.9183, -1.1668,  0.4313],\n",
      "        [-1.6425,  2.3616,  0.4562,  1.9197,  1.9671]])\n",
      "\n",
      "270-degree rotated slice (k=0, c=0, rotation=3):\n",
      " tensor([[-1.6425,  2.3616,  0.4562,  1.9197,  1.9671],\n",
      "        [-0.3668, -1.4771, -2.9183, -1.1668,  0.4313],\n",
      "        [-1.4748,  0.5730, -1.8855,  0.7854,  1.2567],\n",
      "        [ 0.3046,  0.5151, -0.2936,  1.5608, -0.5583],\n",
      "        [-0.5086,  0.1773,  0.4793, -0.7286,  1.1858]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def rotate_4_times(input_tensor):\n",
    "  \"\"\"\n",
    "  Generates a tensor with four clockwise rotations of the last two dimensions.\n",
    "\n",
    "  Args:\n",
    "    input_tensor: A tensor of shape (k, c, q, r).\n",
    "\n",
    "  Returns:\n",
    "    A tensor of shape (k, c, q, 4, r) where the last two dimensions\n",
    "    are rotated clockwise 0, 90, 180, and 270 degrees respectively.\n",
    "  \"\"\"\n",
    "  k, c, q, r = input_tensor.shape\n",
    "  rotated_tensor = torch.zeros(k, c, q, 4, r, dtype=input_tensor.dtype, device=input_tensor.device)\n",
    "\n",
    "  for i in range(k):\n",
    "    for j in range(c):\n",
    "      slice_qr = input_tensor[i, j, :, :]\n",
    "      rotated_tensor[i, j, :, 0, :] = slice_qr  # 0-degree rotation (original)\n",
    "\n",
    "      rotated_90 = torch.rot90(slice_qr, k=-1, dims=(0, 1))\n",
    "      if rotated_90.shape == (r, q):\n",
    "          rotated_tensor[i, j, :q, 1, :r] = rotated_90.transpose(0, 1) # Transpose to (q, r)\n",
    "      else:\n",
    "          raise RuntimeError(f\"Unexpected shape after 90-degree rotation: {rotated_90.shape}\")\n",
    "\n",
    "      rotated_180 = torch.rot90(slice_qr, k=-2, dims=(0, 1))\n",
    "      rotated_tensor[i, j, :, 2, :] = rotated_180 # Shape will be (q, r)\n",
    "\n",
    "      rotated_270 = torch.rot90(slice_qr, k=-3, dims=(0, 1))\n",
    "      if rotated_270.shape == (r, q):\n",
    "          rotated_tensor[i, j, :q, 3, :r] = rotated_270.transpose(0, 1) # Transpose to (q, r)\n",
    "      else:\n",
    "          raise RuntimeError(f\"Unexpected shape after 270-degree rotation: {rotated_270.shape}\")\n",
    "\n",
    "  return rotated_tensor\n",
    "\n",
    "# Example usage:\n",
    "k, c, q, r = 2, 3, 5, 5\n",
    "original_tensor = torch.randn(k, c, q, r)\n",
    "rotated_4_tensor = rotate_4_times(original_tensor)\n",
    "\n",
    "print(\"Original Tensor Shape:\", original_tensor.shape)\n",
    "print(\"Rotated 4 Times Tensor Shape:\", rotated_4_tensor.shape)\n",
    "\n",
    "print(\"\\nOriginal slice (k=0, c=0):\\n\", original_tensor[0, 0])\n",
    "print(\"\\n0-degree rotated slice (k=0, c=0, rotation=0):\\n\", rotated_4_tensor[0, 0, :, 0, :])\n",
    "print(\"\\n90-degree rotated slice (k=0, c=0, rotation=1):\\n\", rotated_4_tensor[0, 0, :, 1, :])\n",
    "print(\"\\n180-degree rotated slice (k=0, c=0, rotation=2):\\n\", rotated_4_tensor[0, 0, :, 2, :])\n",
    "print(\"\\n270-degree rotated slice (k=0, c=0, rotation=3):\\n\", rotated_4_tensor[0, 0, :, 3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d99c10-c781-4de8-a7ef-c1729d0d9382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
