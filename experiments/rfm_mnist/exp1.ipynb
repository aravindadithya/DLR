{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a85af1c7-cc22-4007-a083-f2a298fb67bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93cae0-e62b-4ae4-a7a0-7a7e113b9a9d",
   "metadata": {},
   "source": [
    "This experiment checks the following for a simple 2 layer FC network on MNIST.\n",
    "1. Verify Agop and NFM relations for the conv layers\n",
    "2. Run RFM to construct similar matrices as the above.(TBD)\n",
    "\n",
    "The model is taken from MNIST/model3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1797532-a687-49c7-b3d7-5c5545f2eeaf",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1588384-18ad-4c06-83e7-cb2d7ed5a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "parent_dir='C:\\\\Users\\\\garav\\\\AGOP\\\\DLR'\n",
    "model_dir= 'C:\\\\Users\\\\garav\\\\AGOP\\\\DLR\\\\trained_models\\\\MNIST\\\\model3\\\\nn_models\\\\'\n",
    "#parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "75d858f4-2bd3-43a4-8f4b-88b1978ff657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Without the incoming socket you cannot receive events from the server or register event handlers to your Visdom client.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from utils import trainer as t\n",
    "from utils import agop_fc as af\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import rfm\n",
    "import numpy as np\n",
    "from trained_models.MNIST.model3 import model3\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.linalg import norm\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d028bc-ee51-4fda-9f6b-206b84a1abd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ac3bb17-df96-4419-884c-4d94e643f13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\garav\\\\AGOP\\\\DLR\\\\experiments\\\\rfm_mnist'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8a34574-02da-41c8-a643-f23e872e89b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_data(dataset, num_samples=-1):\n",
    "    labelset = {}\n",
    "    for i in range(10):\n",
    "        one_hot = torch.zeros(10)\n",
    "        one_hot[i] = 1\n",
    "        labelset[i] = one_hot\n",
    "\n",
    "    subset = [(ex.flatten(), labelset[label]) for \\\n",
    "              idx, (ex, label) in enumerate(dataset) if idx < num_samples]\n",
    "    return subset\n",
    "\n",
    "\n",
    "def group_by_class(dataset):\n",
    "    labelset = {}\n",
    "    for i in range(10):\n",
    "        labelset[i] = []\n",
    "    for i, batch in enumerate(dataset):\n",
    "        img, label = batch\n",
    "        labelset[label].append(img.view(1, 3, 32, 32))\n",
    "    return labelset\n",
    "\n",
    "\n",
    "def split(trainset, p=.8):\n",
    "    train, val = train_test_split(trainset, train_size=p)\n",
    "    return train, val\n",
    "\n",
    "def merge_data(mnist, n):\n",
    "    #cifar_by_label = group_by_class(cifar)\n",
    "\n",
    "    mnist_by_label = group_by_class(mnist)\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    labelset = {}\n",
    "\n",
    "    for i in range(10):\n",
    "        one_hot = torch.zeros(1, 10)\n",
    "        one_hot[0, i] = 1\n",
    "        labelset[i] = one_hot\n",
    "\n",
    "    for l in mnist_by_label:\n",
    "\n",
    "        #cifar_data = torch.cat(cifar_by_label[l])\n",
    "        mnist_data = torch.cat(mnist_by_label[l])\n",
    "        min_len = len(mnist_data)\n",
    "        m = min(n, min_len)\n",
    "        #cifar_data = cifar_data[:m]\n",
    "        mnist_data = mnist_data[:m]\n",
    "\n",
    "        merged = torch.cat([mnist_data], axis=-1)\n",
    "        #for i in range(3):\n",
    "           # vis.image(merged[i])\n",
    "        data.append(merged.reshape(m, -1))\n",
    "        print(merged.shape)\n",
    "        labels.append(np.repeat(labelset[l], m, axis=0))\n",
    "    data = torch.cat(data, axis=0)\n",
    "\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    merged_labels = torch.from_numpy(labels)\n",
    "\n",
    "    return list(zip(data, labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851228a2-b59b-41ff-bb5a-fc1af2c3f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d89618-c570-4f80-aa38-3b468e076440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "Train Size:  40000 Val Size:  10000\n",
      "Test Size:  10000\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([892, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SEED = 5700\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "#cudnn.benchmark = False\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "def repeat_channel(x):\n",
    "    return x.repeat(3, 1, 1)\n",
    "\n",
    "mnist_transform = transforms.Compose(\n",
    "    [transforms.Resize([32, 32]),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Lambda(repeat_channel)]\n",
    ")\n",
    "\n",
    "path= './data'  \n",
    "    \n",
    "mnist_trainset = torchvision.datasets.MNIST(root=path,\n",
    "                                                train=True,\n",
    "                                                transform=mnist_transform,\n",
    "                                                download=True)\n",
    "\n",
    "#trainset = group_by_class(mnist_trainset)\n",
    "trainset = merge_data(mnist_trainset, 5000)\n",
    "trainset, valset = split(trainset, p=.8)\n",
    "print(\"Train Size: \", len(trainset), \"Val Size: \", len(valset))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=100,\n",
    "                                            shuffle=False, num_workers=1)\n",
    "\n",
    "\n",
    "mnist_testset = torchvision.datasets.MNIST(root=path,\n",
    "                                               train=False,\n",
    "                                               transform=mnist_transform,\n",
    "                                               download=True)\n",
    "\n",
    "print(\"Test Size: \", len(mnist_testset))\n",
    "testset = merge_data(mnist_testset, 900)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "\n",
    "name = 'mnist_fc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbbaa2e5-f13d-4147-b450-a54773243222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(trainloader):\n",
    "        inputs, labels = batch\n",
    "        _, dim = inputs.shape\n",
    "        break\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33ed0b5d-83af-40b0-870a-286a22512905",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = model3.Net(3072, num_classes=10)\n",
    "init_net=deepcopy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9297bf74-13ae-452d-80fd-086724102a0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully.\n",
      "NUMBER OF PARAMS:  4208660\n",
      "Time:  8.486851453781128\n",
      "Epoch:  0 Train Loss:  0.004660367952892557 Test Loss:  0.006165857172448493 Train Acc:  99.055 Test Acc:  98.09830960854093 Best Val Acc:  98.16 Best Val Loss:  0.006216857014223933 Best Test Acc:  98.09830960854093 Best Test Loss:  0.006165857172448493\n",
      "Time:  8.325209379196167\n",
      "Epoch:  1 Train Loss:  0.004598164322669618 Test Loss:  0.0061226119776173525 Train Acc:  99.0825 Test Acc:  98.09830960854093 Best Val Acc:  98.2 Best Val Loss:  0.006173377558588981 Best Test Acc:  98.09830960854093 Best Test Loss:  0.0061226119776173525\n",
      "Time:  8.168629884719849\n",
      "Epoch:  2 Train Loss:  0.004536297253798694 Test Loss:  0.006063117595966176 Train Acc:  99.11 Test Acc:  98.08718861209964 Best Val Acc:  98.21 Best Val Loss:  0.0061060073459520935 Best Test Acc:  98.08718861209964 Best Test Loss:  0.006063117595966176\n",
      "Time:  8.537569761276245\n",
      "Epoch:  3 Train Loss:  0.004475309145636857 Test Loss:  0.0060445779024745966 Train Acc:  99.1225 Test Acc:  98.09830960854093 Best Val Acc:  98.21 Best Val Loss:  0.006073137756902725 Best Test Acc:  98.08718861209964 Best Test Loss:  0.0060445779024745966\n",
      "Time:  8.25609302520752\n",
      "Epoch:  4 Train Loss:  0.004419802852207794 Test Loss:  0.006236897529603324 Train Acc:  99.115 Test Acc:  98.10943060498221 Best Val Acc:  98.21 Best Val Loss:  0.006073137756902725 Best Test Acc:  98.10943060498221 Best Test Loss:  0.0060445779024745966\n",
      "Time:  8.493781328201294\n",
      "Epoch:  5 Train Loss:  0.004368136106058955 Test Loss:  0.005900733730527059 Train Acc:  99.1725 Test Acc:  98.13167259786478 Best Val Acc:  98.22 Best Val Loss:  0.005932417924050241 Best Test Acc:  98.13167259786478 Best Test Loss:  0.005900733730527059\n",
      "Time:  8.381773471832275\n",
      "Epoch:  6 Train Loss:  0.004308958415640518 Test Loss:  0.005953164236765898 Train Acc:  99.1925 Test Acc:  98.08718861209964 Best Val Acc:  98.29 Best Val Loss:  0.005932417924050241 Best Test Acc:  98.08718861209964 Best Test Loss:  0.005900733730527059\n",
      "Time:  8.732407808303833\n",
      "Epoch:  7 Train Loss:  0.0042546439910074695 Test Loss:  0.005853737463339421 Train Acc:  99.1925 Test Acc:  98.15391459074732 Best Val Acc:  98.29 Best Val Loss:  0.005896318305749446 Best Test Acc:  98.08718861209964 Best Test Loss:  0.005853737463339421\n",
      "Time:  8.487327575683594\n",
      "Epoch:  8 Train Loss:  0.004207727283646818 Test Loss:  0.005823539105141237 Train Acc:  99.175 Test Acc:  98.17615658362989 Best Val Acc:  98.29 Best Val Loss:  0.005875133217778057 Best Test Acc:  98.08718861209964 Best Test Loss:  0.005823539105141237\n",
      "Time:  8.373185157775879\n",
      "Epoch:  9 Train Loss:  0.004154965635971166 Test Loss:  0.00582593441708842 Train Acc:  99.225 Test Acc:  98.16503558718861 Best Val Acc:  98.29 Best Val Loss:  0.005838211225345731 Best Test Acc:  98.16503558718861 Best Test Loss:  0.00582593441708842\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.exists(model_dir+'mnist_fc_trained_nn.pth'):\n",
    "    checkpoint = torch.load(model_dir+'mnist_fc_trained_nn.pth', map_location=torch.device(device))\n",
    "    net.load_state_dict(checkpoint['state_dict'])  # Access the 'state_dict' within the loaded dictionary\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "\n",
    "t.train_network(trainloader, valloader, testloader,\n",
    "                num_classes=10, root_path= model_dir, \n",
    "                optimizer=torch.optim.SGD(net.parameters(), lr=.1),\n",
    "                lfn=  nn.MSELoss(), \n",
    "                num_epochs = 10,\n",
    "                name=name, net=net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339db119-96d3-496f-8f2a-5064a96b8660",
   "metadata": {},
   "source": [
    "# AGOP_FC.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9a58937-1b9c-4823-9f76-630c56997763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Without the incoming socket you cannot receive events from the server or register event handlers to your Visdom client.\n"
     ]
    }
   ],
   "source": [
    "''' This module does the following\n",
    "1. Scan the network for conv layers\n",
    "2. For each FC layer compute W^TW of eq 3\n",
    "3. For each FC layer compute the AGOP(AJOP in case of multiple outputs)\n",
    "4. For each conv layer print the pearson correlation between 2 and 3\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "#from functorch import jacrev, vmap\n",
    "from torch.func import jacrev\n",
    "from torch.nn.functional import pad\n",
    "#import dataset\n",
    "from numpy.linalg import eig\n",
    "from copy import deepcopy\n",
    "from torch.linalg import norm, svd\n",
    "from torchvision import models\n",
    "import visdom\n",
    "from torch.linalg import norm, eig\n",
    "\n",
    "\n",
    "SEED = 2323\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "vis = visdom.Visdom('http://127.0.0.1', use_incoming_socket=False)\n",
    "vis.close(env='main')\n",
    "\n",
    "def get_jacobian(net, data, c_idx=0, chunk=100):\n",
    "    with torch.no_grad():\n",
    "        def single_net(x):\n",
    "            # x is (s)\n",
    "            return net(x.unsqueeze(0))[:,c_idx*chunk:(c_idx+1)*chunk].squeeze(0)\n",
    "        # Parallelize across the images.\n",
    "        return torch.vmap(jacrev(single_net))(data) #(n,chunk,s)\n",
    "\n",
    "def min_max(M):\n",
    "    return (M - M.min()) / (M.max() - M.min())\n",
    "\n",
    "def sqrt(G):\n",
    "    U, s, Vt = svd(G)\n",
    "    s = torch.pow(s, 1./2)\n",
    "    G = U @ torch.diag(s) @ Vt\n",
    "    return G\n",
    "\n",
    "\n",
    "def correlation(M1, M2):\n",
    "    M1 -= M1.mean()\n",
    "    M2 -= M2.mean()\n",
    "\n",
    "    norm1 = norm(M1.flatten())\n",
    "    norm2 = norm(M2.flatten())\n",
    "\n",
    "    return torch.sum(M1.cuda() * M2.cuda()) / (norm1 * norm2)\n",
    "\n",
    "def egop(model, z, c=10, chunk_idxs=1):\n",
    "    ajop = 0\n",
    "    #Chunking is done to compute jacobian as chunks. This saves memory\n",
    "    chunk = c // chunk_idxs\n",
    "    for i in range(chunk_idxs):\n",
    "        grads = get_jacobian(model, z, c_idx=i, chunk=chunk) #(n,chunk,s)\n",
    "        grads_t = grads.transpose(1, 2) \n",
    "        ajop_matmul= torch.matmul(grads_t, grads) #(n,s,s)\n",
    "        #Clarify: mean and sum are making no difference here. Check if trainloader has grouped images\n",
    "        ajop += torch.mean(ajop_matmul, dim=0) #(s,s)\n",
    "    return ajop\n",
    "\n",
    "\n",
    "\n",
    "def get_grads(net, patchnet, trainloader, max_batch, classes, chunk_idx,\n",
    "              kernel=(3,3), padding=(1,1),\n",
    "              stride=(1,1), layer_idx=0):\n",
    "    net.eval()\n",
    "    net.cuda()\n",
    "    patchnet.eval()\n",
    "    patchnet.cuda()\n",
    "    M = 0\n",
    "    #M.cuda()\n",
    "    \n",
    "    # Num images for taking AGOP (Can be small for early layers)\n",
    "    MAX_NUM_IMGS = max_batch\n",
    "\n",
    "    for idx, batch in enumerate(trainloader):\n",
    "        print(\"Computing GOP for sample \" + str(idx) + \\\n",
    "              \" out of \" + str(MAX_NUM_IMGS))\n",
    "        imgs, _ = batch\n",
    "        #imgs=imgs[:]\n",
    "        with torch.no_grad():\n",
    "            imgs = imgs.cuda()        \n",
    "            # Run the first half of the network wrt to the current layer \n",
    "            ip = net.features[:layer_idx](imgs).cpu() #(n,s)\n",
    "            \n",
    "        #print(patches.shape)\n",
    "        M += egop(patchnet,ip.cuda(), classes, chunk_idx).cuda()\n",
    "        del imgs\n",
    "        torch.cuda.empty_cache()\n",
    "        if idx >= MAX_NUM_IMGS:\n",
    "            break\n",
    "    net.cpu()\n",
    "    patchnet.cpu()\n",
    "    return M\n",
    "\n",
    "def load_nn(net, init_net, layer_idx=0):\n",
    "   \n",
    "    count = 0\n",
    "    \n",
    "    # Get the layer_idx+1 th conv layer\n",
    "    #TODO: Add functionality to access classifier layers too.\n",
    "    for idx, m in enumerate(net.features):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            count += 1\n",
    "        if count-1 == layer_idx:\n",
    "            l_idx = idx\n",
    "            break\n",
    "    \n",
    "    patchnet = deepcopy(net)\n",
    "    \n",
    "    # Truncate all layers before l_idx.\n",
    "    patchnet.features = net.features[l_idx:]\n",
    "    \n",
    "    M = net.features[l_idx].weight.data\n",
    "    # Compute WW which is (s,s) matrix\n",
    "    M =torch.matmul(M.T, M)\n",
    "    M0 = init_net.features[l_idx].weight.data\n",
    "    # Compute W0tW0 which is (s,s) matrix\n",
    "    M0 =torch.matmul(M0.T, M0)\n",
    "    return net, patchnet, M, M0, l_idx\n",
    "\n",
    "\n",
    "def verify_NFA(net, init_net, trainloader, layer_idx=0, max_batch=10, classes=10, chunk_idx=1):\n",
    "\n",
    "\n",
    "    net, patchnet, M, M0, l_idx = load_nn(net, init_net, layer_idx=layer_idx)\n",
    "\n",
    "    i_val = correlation(M0.cuda(), M.cuda())\n",
    "    print(\"Correlation between Initial and Trained CNFM: \", i_val)\n",
    "\n",
    "    G = get_grads(net, patchnet, trainloader,  max_batch, classes, chunk_idx,\n",
    "                  layer_idx=l_idx)\n",
    "    print(\"Shape of grad matrix\",G.shape)\n",
    "    G = sqrt(G.cuda())\n",
    "    Gop = G.clone()\n",
    "    r_val = correlation(M.cuda(), G.cuda())\n",
    "    print(\"Correlation between Trained CNFM and AGOP: \", r_val)\n",
    "    print(\"Final: \", i_val, r_val)\n",
    "    return Gop\n",
    "\n",
    "def vis_transform_image(net, img, G, layer_idx=0):\n",
    "   #TODO: What to visualise for the FC layers?\n",
    "    count = -1\n",
    "    \n",
    "    # Computes WtW for the weights(ignoring its bias) of layer_idx+1 the conv layer\n",
    "    for idx, p in enumerate(net.parameters()):\n",
    "        if len(p.shape) > 1:\n",
    "            count += 1\n",
    "        if count == layer_idx:\n",
    "            M = p.data\n",
    "            _, ki, q, s = M.shape\n",
    "\n",
    "            M = M.reshape(-1, ki*q*s)\n",
    "            M = torch.einsum('nd, nD -> dD', M, M)\n",
    "            break\n",
    "\n",
    "    count = 0\n",
    "    l_idx = None\n",
    "    \n",
    "    # Get the layer_idx+1 conv layer \n",
    "    for idx, m in enumerate(net.features):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            print(m, count)\n",
    "            count += 1\n",
    "\n",
    "        if count-1 == layer_idx:\n",
    "            l_idx = idx\n",
    "            break\n",
    "\n",
    "    net.eval()\n",
    "    net.cuda()\n",
    "    img = img.cuda()\n",
    "    img = net.features[:l_idx](img).cpu()\n",
    "    net.cpu()\n",
    "    \n",
    "    # If G is given which is expected to be the AGOP of layer_idx+1 conv layer then that is used.\n",
    "    if G is not None:\n",
    "        M = G\n",
    "\n",
    "    patches = patchify(img, (q, s), (1, 1))\n",
    "    \n",
    "    print(patches.shape)\n",
    "    # Patches should will be of the shape (n,w,h,c,q,s) not (n,w,h,q,s,c)\n",
    "    n, w, h, q, s, c = patches.shape\n",
    "    # Vectorize each patch\n",
    "    patches = patches.reshape(n, w, h, q*s*c)\n",
    "    # Apply either WtW or AGOP of the layer_idx+1 conv to each patch. D is c*q*s vector\n",
    "    M_patch = torch.einsum('nwhd, dD -> nwhD', patches, M) #(n,w,h,c*q*s)\n",
    "    \n",
    "    M_patch = norm(M_patch, dim=-1) #(n,w,h)\n",
    "\n",
    "    vis.image(min_max(M_patch[0])) #(w,h) image.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87258fbe-c4b8-49d9-8319-74b5a72a4584",
   "metadata": {},
   "source": [
    "# Verify NFA for FC layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "206e8d75-2d15-4529-9bdf-325264efc42b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (features): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=1024, bias=True)\n",
      "    (1): Nonlinearity()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): Nonlinearity()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Correlation between Initial and Trained CNFM:  tensor(0.2363, device='cuda:0')\n",
      "Computing GOP for sample 0 out of 1000\n",
      "Computing GOP for sample 1 out of 1000\n",
      "Computing GOP for sample 2 out of 1000\n",
      "Computing GOP for sample 3 out of 1000\n",
      "Computing GOP for sample 4 out of 1000\n",
      "Computing GOP for sample 5 out of 1000\n",
      "Computing GOP for sample 6 out of 1000\n",
      "Computing GOP for sample 7 out of 1000\n",
      "Computing GOP for sample 8 out of 1000\n",
      "Computing GOP for sample 9 out of 1000\n",
      "Computing GOP for sample 10 out of 1000\n",
      "Computing GOP for sample 11 out of 1000\n",
      "Computing GOP for sample 12 out of 1000\n",
      "Computing GOP for sample 13 out of 1000\n",
      "Computing GOP for sample 14 out of 1000\n",
      "Computing GOP for sample 15 out of 1000\n",
      "Computing GOP for sample 16 out of 1000\n",
      "Computing GOP for sample 17 out of 1000\n",
      "Computing GOP for sample 18 out of 1000\n",
      "Computing GOP for sample 19 out of 1000\n",
      "Computing GOP for sample 20 out of 1000\n",
      "Computing GOP for sample 21 out of 1000\n",
      "Computing GOP for sample 22 out of 1000\n",
      "Computing GOP for sample 23 out of 1000\n",
      "Computing GOP for sample 24 out of 1000\n",
      "Computing GOP for sample 25 out of 1000\n",
      "Computing GOP for sample 26 out of 1000\n",
      "Computing GOP for sample 27 out of 1000\n",
      "Computing GOP for sample 28 out of 1000\n",
      "Computing GOP for sample 29 out of 1000\n",
      "Computing GOP for sample 30 out of 1000\n",
      "Computing GOP for sample 31 out of 1000\n",
      "Computing GOP for sample 32 out of 1000\n",
      "Computing GOP for sample 33 out of 1000\n",
      "Computing GOP for sample 34 out of 1000\n",
      "Computing GOP for sample 35 out of 1000\n",
      "Computing GOP for sample 36 out of 1000\n",
      "Computing GOP for sample 37 out of 1000\n",
      "Computing GOP for sample 38 out of 1000\n",
      "Computing GOP for sample 39 out of 1000\n",
      "Computing GOP for sample 40 out of 1000\n",
      "Computing GOP for sample 41 out of 1000\n",
      "Computing GOP for sample 42 out of 1000\n",
      "Computing GOP for sample 43 out of 1000\n",
      "Computing GOP for sample 44 out of 1000\n",
      "Computing GOP for sample 45 out of 1000\n",
      "Computing GOP for sample 46 out of 1000\n",
      "Computing GOP for sample 47 out of 1000\n",
      "Computing GOP for sample 48 out of 1000\n",
      "Computing GOP for sample 49 out of 1000\n",
      "Computing GOP for sample 50 out of 1000\n",
      "Computing GOP for sample 51 out of 1000\n",
      "Computing GOP for sample 52 out of 1000\n",
      "Computing GOP for sample 53 out of 1000\n",
      "Computing GOP for sample 54 out of 1000\n",
      "Computing GOP for sample 55 out of 1000\n",
      "Computing GOP for sample 56 out of 1000\n",
      "Computing GOP for sample 57 out of 1000\n",
      "Computing GOP for sample 58 out of 1000\n",
      "Computing GOP for sample 59 out of 1000\n",
      "Computing GOP for sample 60 out of 1000\n",
      "Computing GOP for sample 61 out of 1000\n",
      "Computing GOP for sample 62 out of 1000\n",
      "Computing GOP for sample 63 out of 1000\n",
      "Computing GOP for sample 64 out of 1000\n",
      "Computing GOP for sample 65 out of 1000\n",
      "Computing GOP for sample 66 out of 1000\n",
      "Computing GOP for sample 67 out of 1000\n",
      "Computing GOP for sample 68 out of 1000\n",
      "Computing GOP for sample 69 out of 1000\n",
      "Computing GOP for sample 70 out of 1000\n",
      "Computing GOP for sample 71 out of 1000\n",
      "Computing GOP for sample 72 out of 1000\n",
      "Computing GOP for sample 73 out of 1000\n",
      "Computing GOP for sample 74 out of 1000\n",
      "Computing GOP for sample 75 out of 1000\n",
      "Computing GOP for sample 76 out of 1000\n",
      "Computing GOP for sample 77 out of 1000\n",
      "Computing GOP for sample 78 out of 1000\n",
      "Computing GOP for sample 79 out of 1000\n",
      "Computing GOP for sample 80 out of 1000\n",
      "Computing GOP for sample 81 out of 1000\n",
      "Computing GOP for sample 82 out of 1000\n",
      "Computing GOP for sample 83 out of 1000\n",
      "Computing GOP for sample 84 out of 1000\n",
      "Computing GOP for sample 85 out of 1000\n",
      "Computing GOP for sample 86 out of 1000\n",
      "Computing GOP for sample 87 out of 1000\n",
      "Computing GOP for sample 88 out of 1000\n",
      "Computing GOP for sample 89 out of 1000\n",
      "Computing GOP for sample 90 out of 1000\n",
      "Computing GOP for sample 91 out of 1000\n",
      "Computing GOP for sample 92 out of 1000\n",
      "Computing GOP for sample 93 out of 1000\n",
      "Computing GOP for sample 94 out of 1000\n",
      "Computing GOP for sample 95 out of 1000\n",
      "Computing GOP for sample 96 out of 1000\n",
      "Computing GOP for sample 97 out of 1000\n",
      "Computing GOP for sample 98 out of 1000\n",
      "Computing GOP for sample 99 out of 1000\n",
      "Computing GOP for sample 100 out of 1000\n",
      "Computing GOP for sample 101 out of 1000\n",
      "Computing GOP for sample 102 out of 1000\n",
      "Computing GOP for sample 103 out of 1000\n",
      "Computing GOP for sample 104 out of 1000\n",
      "Computing GOP for sample 105 out of 1000\n",
      "Computing GOP for sample 106 out of 1000\n",
      "Computing GOP for sample 107 out of 1000\n",
      "Computing GOP for sample 108 out of 1000\n",
      "Computing GOP for sample 109 out of 1000\n",
      "Computing GOP for sample 110 out of 1000\n",
      "Computing GOP for sample 111 out of 1000\n",
      "Computing GOP for sample 112 out of 1000\n",
      "Computing GOP for sample 113 out of 1000\n",
      "Computing GOP for sample 114 out of 1000\n",
      "Computing GOP for sample 115 out of 1000\n",
      "Computing GOP for sample 116 out of 1000\n",
      "Computing GOP for sample 117 out of 1000\n",
      "Computing GOP for sample 118 out of 1000\n",
      "Computing GOP for sample 119 out of 1000\n",
      "Computing GOP for sample 120 out of 1000\n",
      "Computing GOP for sample 121 out of 1000\n",
      "Computing GOP for sample 122 out of 1000\n",
      "Computing GOP for sample 123 out of 1000\n",
      "Computing GOP for sample 124 out of 1000\n",
      "Computing GOP for sample 125 out of 1000\n",
      "Computing GOP for sample 126 out of 1000\n",
      "Computing GOP for sample 127 out of 1000\n",
      "Computing GOP for sample 128 out of 1000\n",
      "Computing GOP for sample 129 out of 1000\n",
      "Computing GOP for sample 130 out of 1000\n",
      "Computing GOP for sample 131 out of 1000\n",
      "Computing GOP for sample 132 out of 1000\n",
      "Computing GOP for sample 133 out of 1000\n",
      "Computing GOP for sample 134 out of 1000\n",
      "Computing GOP for sample 135 out of 1000\n",
      "Computing GOP for sample 136 out of 1000\n",
      "Computing GOP for sample 137 out of 1000\n",
      "Computing GOP for sample 138 out of 1000\n",
      "Computing GOP for sample 139 out of 1000\n",
      "Computing GOP for sample 140 out of 1000\n",
      "Computing GOP for sample 141 out of 1000\n",
      "Computing GOP for sample 142 out of 1000\n",
      "Computing GOP for sample 143 out of 1000\n",
      "Computing GOP for sample 144 out of 1000\n",
      "Computing GOP for sample 145 out of 1000\n",
      "Computing GOP for sample 146 out of 1000\n",
      "Computing GOP for sample 147 out of 1000\n",
      "Computing GOP for sample 148 out of 1000\n",
      "Computing GOP for sample 149 out of 1000\n",
      "Computing GOP for sample 150 out of 1000\n",
      "Computing GOP for sample 151 out of 1000\n",
      "Computing GOP for sample 152 out of 1000\n",
      "Computing GOP for sample 153 out of 1000\n",
      "Computing GOP for sample 154 out of 1000\n",
      "Computing GOP for sample 155 out of 1000\n",
      "Computing GOP for sample 156 out of 1000\n",
      "Computing GOP for sample 157 out of 1000\n",
      "Computing GOP for sample 158 out of 1000\n",
      "Computing GOP for sample 159 out of 1000\n",
      "Computing GOP for sample 160 out of 1000\n",
      "Computing GOP for sample 161 out of 1000\n",
      "Computing GOP for sample 162 out of 1000\n",
      "Computing GOP for sample 163 out of 1000\n",
      "Computing GOP for sample 164 out of 1000\n",
      "Computing GOP for sample 165 out of 1000\n",
      "Computing GOP for sample 166 out of 1000\n",
      "Computing GOP for sample 167 out of 1000\n",
      "Computing GOP for sample 168 out of 1000\n",
      "Computing GOP for sample 169 out of 1000\n",
      "Computing GOP for sample 170 out of 1000\n",
      "Computing GOP for sample 171 out of 1000\n",
      "Computing GOP for sample 172 out of 1000\n",
      "Computing GOP for sample 173 out of 1000\n",
      "Computing GOP for sample 174 out of 1000\n",
      "Computing GOP for sample 175 out of 1000\n",
      "Computing GOP for sample 176 out of 1000\n",
      "Computing GOP for sample 177 out of 1000\n",
      "Computing GOP for sample 178 out of 1000\n",
      "Computing GOP for sample 179 out of 1000\n",
      "Computing GOP for sample 180 out of 1000\n",
      "Computing GOP for sample 181 out of 1000\n",
      "Computing GOP for sample 182 out of 1000\n",
      "Computing GOP for sample 183 out of 1000\n",
      "Computing GOP for sample 184 out of 1000\n",
      "Computing GOP for sample 185 out of 1000\n",
      "Computing GOP for sample 186 out of 1000\n",
      "Computing GOP for sample 187 out of 1000\n",
      "Computing GOP for sample 188 out of 1000\n",
      "Computing GOP for sample 189 out of 1000\n",
      "Computing GOP for sample 190 out of 1000\n",
      "Computing GOP for sample 191 out of 1000\n",
      "Computing GOP for sample 192 out of 1000\n",
      "Computing GOP for sample 193 out of 1000\n",
      "Computing GOP for sample 194 out of 1000\n",
      "Computing GOP for sample 195 out of 1000\n",
      "Computing GOP for sample 196 out of 1000\n",
      "Computing GOP for sample 197 out of 1000\n",
      "Computing GOP for sample 198 out of 1000\n",
      "Computing GOP for sample 199 out of 1000\n",
      "Computing GOP for sample 200 out of 1000\n",
      "Computing GOP for sample 201 out of 1000\n",
      "Computing GOP for sample 202 out of 1000\n",
      "Computing GOP for sample 203 out of 1000\n",
      "Computing GOP for sample 204 out of 1000\n",
      "Computing GOP for sample 205 out of 1000\n",
      "Computing GOP for sample 206 out of 1000\n",
      "Computing GOP for sample 207 out of 1000\n",
      "Computing GOP for sample 208 out of 1000\n",
      "Computing GOP for sample 209 out of 1000\n",
      "Computing GOP for sample 210 out of 1000\n",
      "Computing GOP for sample 211 out of 1000\n",
      "Computing GOP for sample 212 out of 1000\n",
      "Computing GOP for sample 213 out of 1000\n",
      "Computing GOP for sample 214 out of 1000\n",
      "Computing GOP for sample 215 out of 1000\n",
      "Computing GOP for sample 216 out of 1000\n",
      "Computing GOP for sample 217 out of 1000\n",
      "Computing GOP for sample 218 out of 1000\n",
      "Computing GOP for sample 219 out of 1000\n",
      "Computing GOP for sample 220 out of 1000\n",
      "Computing GOP for sample 221 out of 1000\n",
      "Computing GOP for sample 222 out of 1000\n",
      "Computing GOP for sample 223 out of 1000\n",
      "Computing GOP for sample 224 out of 1000\n",
      "Computing GOP for sample 225 out of 1000\n",
      "Computing GOP for sample 226 out of 1000\n",
      "Computing GOP for sample 227 out of 1000\n",
      "Computing GOP for sample 228 out of 1000\n",
      "Computing GOP for sample 229 out of 1000\n",
      "Computing GOP for sample 230 out of 1000\n",
      "Computing GOP for sample 231 out of 1000\n",
      "Computing GOP for sample 232 out of 1000\n",
      "Computing GOP for sample 233 out of 1000\n",
      "Computing GOP for sample 234 out of 1000\n",
      "Computing GOP for sample 235 out of 1000\n",
      "Computing GOP for sample 236 out of 1000\n",
      "Computing GOP for sample 237 out of 1000\n",
      "Computing GOP for sample 238 out of 1000\n",
      "Computing GOP for sample 239 out of 1000\n",
      "Computing GOP for sample 240 out of 1000\n",
      "Computing GOP for sample 241 out of 1000\n",
      "Computing GOP for sample 242 out of 1000\n",
      "Computing GOP for sample 243 out of 1000\n",
      "Computing GOP for sample 244 out of 1000\n",
      "Computing GOP for sample 245 out of 1000\n",
      "Computing GOP for sample 246 out of 1000\n",
      "Computing GOP for sample 247 out of 1000\n",
      "Computing GOP for sample 248 out of 1000\n",
      "Computing GOP for sample 249 out of 1000\n",
      "Computing GOP for sample 250 out of 1000\n",
      "Computing GOP for sample 251 out of 1000\n",
      "Computing GOP for sample 252 out of 1000\n",
      "Computing GOP for sample 253 out of 1000\n",
      "Computing GOP for sample 254 out of 1000\n",
      "Computing GOP for sample 255 out of 1000\n",
      "Computing GOP for sample 256 out of 1000\n",
      "Computing GOP for sample 257 out of 1000\n",
      "Computing GOP for sample 258 out of 1000\n",
      "Computing GOP for sample 259 out of 1000\n",
      "Computing GOP for sample 260 out of 1000\n",
      "Computing GOP for sample 261 out of 1000\n",
      "Computing GOP for sample 262 out of 1000\n",
      "Computing GOP for sample 263 out of 1000\n",
      "Computing GOP for sample 264 out of 1000\n",
      "Computing GOP for sample 265 out of 1000\n",
      "Computing GOP for sample 266 out of 1000\n",
      "Computing GOP for sample 267 out of 1000\n",
      "Computing GOP for sample 268 out of 1000\n",
      "Computing GOP for sample 269 out of 1000\n",
      "Computing GOP for sample 270 out of 1000\n",
      "Computing GOP for sample 271 out of 1000\n",
      "Computing GOP for sample 272 out of 1000\n",
      "Computing GOP for sample 273 out of 1000\n",
      "Computing GOP for sample 274 out of 1000\n",
      "Computing GOP for sample 275 out of 1000\n",
      "Computing GOP for sample 276 out of 1000\n",
      "Computing GOP for sample 277 out of 1000\n",
      "Computing GOP for sample 278 out of 1000\n",
      "Computing GOP for sample 279 out of 1000\n",
      "Computing GOP for sample 280 out of 1000\n",
      "Computing GOP for sample 281 out of 1000\n",
      "Computing GOP for sample 282 out of 1000\n",
      "Computing GOP for sample 283 out of 1000\n",
      "Computing GOP for sample 284 out of 1000\n",
      "Computing GOP for sample 285 out of 1000\n",
      "Computing GOP for sample 286 out of 1000\n",
      "Computing GOP for sample 287 out of 1000\n",
      "Computing GOP for sample 288 out of 1000\n",
      "Computing GOP for sample 289 out of 1000\n",
      "Computing GOP for sample 290 out of 1000\n",
      "Computing GOP for sample 291 out of 1000\n",
      "Computing GOP for sample 292 out of 1000\n",
      "Computing GOP for sample 293 out of 1000\n",
      "Computing GOP for sample 294 out of 1000\n",
      "Computing GOP for sample 295 out of 1000\n",
      "Computing GOP for sample 296 out of 1000\n",
      "Computing GOP for sample 297 out of 1000\n",
      "Computing GOP for sample 298 out of 1000\n",
      "Computing GOP for sample 299 out of 1000\n",
      "Computing GOP for sample 300 out of 1000\n",
      "Computing GOP for sample 301 out of 1000\n",
      "Computing GOP for sample 302 out of 1000\n",
      "Computing GOP for sample 303 out of 1000\n",
      "Computing GOP for sample 304 out of 1000\n",
      "Computing GOP for sample 305 out of 1000\n",
      "Computing GOP for sample 306 out of 1000\n",
      "Computing GOP for sample 307 out of 1000\n",
      "Computing GOP for sample 308 out of 1000\n",
      "Computing GOP for sample 309 out of 1000\n",
      "Computing GOP for sample 310 out of 1000\n",
      "Computing GOP for sample 311 out of 1000\n",
      "Computing GOP for sample 312 out of 1000\n",
      "Computing GOP for sample 313 out of 1000\n",
      "Computing GOP for sample 314 out of 1000\n",
      "Computing GOP for sample 315 out of 1000\n",
      "Computing GOP for sample 316 out of 1000\n",
      "Computing GOP for sample 317 out of 1000\n",
      "Computing GOP for sample 318 out of 1000\n",
      "Computing GOP for sample 319 out of 1000\n",
      "Computing GOP for sample 320 out of 1000\n",
      "Computing GOP for sample 321 out of 1000\n",
      "Computing GOP for sample 322 out of 1000\n",
      "Computing GOP for sample 323 out of 1000\n",
      "Computing GOP for sample 324 out of 1000\n",
      "Computing GOP for sample 325 out of 1000\n",
      "Computing GOP for sample 326 out of 1000\n",
      "Computing GOP for sample 327 out of 1000\n",
      "Computing GOP for sample 328 out of 1000\n",
      "Computing GOP for sample 329 out of 1000\n",
      "Computing GOP for sample 330 out of 1000\n",
      "Computing GOP for sample 331 out of 1000\n",
      "Computing GOP for sample 332 out of 1000\n",
      "Computing GOP for sample 333 out of 1000\n",
      "Computing GOP for sample 334 out of 1000\n",
      "Computing GOP for sample 335 out of 1000\n",
      "Computing GOP for sample 336 out of 1000\n",
      "Computing GOP for sample 337 out of 1000\n",
      "Computing GOP for sample 338 out of 1000\n",
      "Computing GOP for sample 339 out of 1000\n",
      "Computing GOP for sample 340 out of 1000\n",
      "Computing GOP for sample 341 out of 1000\n",
      "Computing GOP for sample 342 out of 1000\n",
      "Computing GOP for sample 343 out of 1000\n",
      "Computing GOP for sample 344 out of 1000\n",
      "Computing GOP for sample 345 out of 1000\n",
      "Computing GOP for sample 346 out of 1000\n",
      "Computing GOP for sample 347 out of 1000\n",
      "Computing GOP for sample 348 out of 1000\n",
      "Computing GOP for sample 349 out of 1000\n",
      "Computing GOP for sample 350 out of 1000\n",
      "Computing GOP for sample 351 out of 1000\n",
      "Computing GOP for sample 352 out of 1000\n",
      "Computing GOP for sample 353 out of 1000\n",
      "Computing GOP for sample 354 out of 1000\n",
      "Computing GOP for sample 355 out of 1000\n",
      "Computing GOP for sample 356 out of 1000\n",
      "Computing GOP for sample 357 out of 1000\n",
      "Computing GOP for sample 358 out of 1000\n",
      "Computing GOP for sample 359 out of 1000\n",
      "Computing GOP for sample 360 out of 1000\n",
      "Computing GOP for sample 361 out of 1000\n",
      "Computing GOP for sample 362 out of 1000\n",
      "Computing GOP for sample 363 out of 1000\n",
      "Computing GOP for sample 364 out of 1000\n",
      "Computing GOP for sample 365 out of 1000\n",
      "Computing GOP for sample 366 out of 1000\n",
      "Computing GOP for sample 367 out of 1000\n",
      "Computing GOP for sample 368 out of 1000\n",
      "Computing GOP for sample 369 out of 1000\n",
      "Computing GOP for sample 370 out of 1000\n",
      "Computing GOP for sample 371 out of 1000\n",
      "Computing GOP for sample 372 out of 1000\n",
      "Computing GOP for sample 373 out of 1000\n",
      "Computing GOP for sample 374 out of 1000\n",
      "Computing GOP for sample 375 out of 1000\n",
      "Computing GOP for sample 376 out of 1000\n",
      "Computing GOP for sample 377 out of 1000\n",
      "Computing GOP for sample 378 out of 1000\n",
      "Computing GOP for sample 379 out of 1000\n",
      "Computing GOP for sample 380 out of 1000\n",
      "Computing GOP for sample 381 out of 1000\n",
      "Computing GOP for sample 382 out of 1000\n",
      "Computing GOP for sample 383 out of 1000\n",
      "Computing GOP for sample 384 out of 1000\n",
      "Computing GOP for sample 385 out of 1000\n",
      "Computing GOP for sample 386 out of 1000\n",
      "Computing GOP for sample 387 out of 1000\n",
      "Computing GOP for sample 388 out of 1000\n",
      "Computing GOP for sample 389 out of 1000\n",
      "Computing GOP for sample 390 out of 1000\n",
      "Computing GOP for sample 391 out of 1000\n",
      "Computing GOP for sample 392 out of 1000\n",
      "Computing GOP for sample 393 out of 1000\n",
      "Computing GOP for sample 394 out of 1000\n",
      "Computing GOP for sample 395 out of 1000\n",
      "Computing GOP for sample 396 out of 1000\n",
      "Computing GOP for sample 397 out of 1000\n",
      "Computing GOP for sample 398 out of 1000\n",
      "Computing GOP for sample 399 out of 1000\n",
      "Shape of grad matrix torch.Size([3072, 3072])\n",
      "Correlation between Trained CNFM and AGOP:  tensor(0.7137, device='cuda:0')\n",
      "Final:  tensor(0.2363, device='cuda:0') tensor(0.7137, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1003, -0.0017,  0.0037,  ..., -0.0082,  0.0033,  0.0003],\n",
       "        [-0.0017,  0.1118, -0.0074,  ...,  0.0074, -0.0028, -0.0011],\n",
       "        [ 0.0037, -0.0074,  0.1026,  ..., -0.0043,  0.0043, -0.0027],\n",
       "        ...,\n",
       "        [-0.0082,  0.0074, -0.0043,  ...,  0.1148,  0.0028,  0.0003],\n",
       "        [ 0.0033, -0.0028,  0.0043,  ...,  0.0028,  0.1068,  0.0020],\n",
       "        [ 0.0003, -0.0011, -0.0027,  ...,  0.0003,  0.0020,  0.1089]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(net)\n",
    "af.verify_NFA(net, init_net, trainloader, max_batch= 1000, classes=10, chunk_idx=1, layer_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0696c3cb-e6fa-4e9f-b222-f1fa2845af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: How to meaningfully visualise? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb970f08-c3a0-405d-8ac3-f3311e2f88f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# RFM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fdd49e-4722-4f2d-887e-df6ad9b1c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Warning: This is an extremely cpu intensive process since it uses solve function from linalg \n",
    "The rfm.py from utils is equipped with more memory efficient solvers. \n",
    "'''\n",
    "\n",
    "rfm.rfm(trainloader, valloader, testloader, name=name,\n",
    "            batch_size=10, iters=1, reg=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c26017-64bb-4682-8271-a349255271c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
