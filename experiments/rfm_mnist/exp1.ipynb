{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a85af1c7-cc22-4007-a083-f2a298fb67bf",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93cae0-e62b-4ae4-a7a0-7a7e113b9a9d",
   "metadata": {},
   "source": [
    "This experiment checks the following for a simple 2 layer FC network on MNIST.\n",
    "1. Verify Agop and NFM relations for the conv layers\n",
    "2. Run RFM to construct similar matrices as the above.(TBD)\n",
    "\n",
    "The model is taken from MNIST/model3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1797532-a687-49c7-b3d7-5c5545f2eeaf",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f50b39-e1de-4fb7-9b39-f96c3d312662",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --branch master_colab https://github.com/aravindadithya/DLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c3e21-f9e5-4e16-8cbb-d934e50552e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install visdom\n",
    "#python -m visdom.server -port 8097"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85933076-65bc-4a74-98ae-c2bd77aae52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hickle\n",
      "  Downloading hickle-5.0.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting h5py>=2.10.0 (from hickle)\n",
      "  Downloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: numpy!=1.20,>=1.8 in /usr/local/lib/python3.12/dist-packages (from hickle) (2.3.1)\n",
      "Downloading hickle-5.0.3-py3-none-any.whl (107 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: h5py, hickle\n",
      "Successfully installed h5py-3.14.0 hickle-5.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install hickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1588384-18ad-4c06-83e7-cb2d7ed5a692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garav\\AGOP\\DLR\\experiments\\rfm_mnist\n",
      "Parent directory: C:\\Users\\garav\\AGOP\\DLR\n",
      "Model directory 1: C:\\Users\\garav\\AGOP\\DLR\\trained_models\\MNIST\\model1\\nn_models/\n",
      "Model directory 3: C:\\Users\\garav\\AGOP\\DLR\\trained_models\\MNIST\\model3\\nn_models/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "\n",
    "parent_dir = os.path.dirname(os.path.dirname(current_dir))\n",
    "model_dir1 = os.path.join(parent_dir, 'trained_models', 'MNIST', 'model1', 'nn_models/')\n",
    "model_dir3 = os.path.join(parent_dir, 'trained_models', 'MNIST', 'model3', 'nn_models/')\n",
    "\n",
    "'''\n",
    "parent_dir='C:\\\\Users\\\\garav\\\\AGOP\\\\DLR'\n",
    "model_dir1= 'C:\\\\Users\\\\garav\\\\AGOP\\\\DLR\\\\trained_models\\\\MNIST\\\\model1\\\\nn_models\\\\'\n",
    "model_dir3= 'C:\\\\Users\\\\garav\\\\AGOP\\\\DLR\\\\trained_models\\\\MNIST\\\\model3\\\\nn_models\\\\'\n",
    "#parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "'''\n",
    "sys.path.append(parent_dir)\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "print(f\"Model directory 1: {model_dir1}\")\n",
    "print(f\"Model directory 3: {model_dir3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d858f4-2bd3-43a4-8f4b-88b1978ff657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Without the incoming socket you cannot receive events from the server or register event handlers to your Visdom client.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/DLR/experiments/rfm_mnist\n",
      "/workspace/DLR\n",
      "/workspace/DLR/trained_models/MNIST/model1/nn_models/\n",
      "/workspace/DLR/experiments/rfm_mnist\n",
      "/workspace/DLR\n",
      "/workspace/DLR/trained_models/MNIST/model3/nn_models/\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from utils import agop_fc as af\n",
    "from utils import agop_fc1 as af1\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import rfm\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "from trained_models.MNIST.model1 import trainer as t1\n",
    "from trained_models.MNIST.model3 import trainer as t3\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.linalg import norm\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d028bc-ee51-4fda-9f6b-206b84a1abd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851228a2-b59b-41ff-bb5a-fc1af2c3f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f3def1b-9d25-467e-ba0b-bfcd69fcdd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "Train Size:  40000 Val Size:  10000\n",
      "Test Size:  10000\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([892, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "Model weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "trainloader, valloader, testloader = t1.get_loaders()\n",
    "net= t1.get_untrained_net()\n",
    "init_net= deepcopy(net)\n",
    "import os\n",
    "if os.path.exists(model_dir1+'mnist_fc_trained_nn.pth'):\n",
    "    checkpoint = torch.load(model_dir1+'mnist_fc_trained_nn.pth', map_location=torch.device(device))\n",
    "    net.load_state_dict(checkpoint['state_dict'])  # Access the 'state_dict' within the loaded dictionary\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "else:\n",
    "    print(\"Train network first\")\n",
    "\n",
    "#print(\"Train the network first\")\n",
    "#t1.train_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b23854c-2ed3-4576-b498-fc5093c410ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "torch.Size([5000, 3, 32, 32])\n",
      "Train Size:  40000 Val Size:  10000\n",
      "Test Size:  10000\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([892, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "torch.Size([900, 3, 32, 32])\n",
      "Model weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "trainloader, valloader, testloader = t3.get_loaders()\n",
    "net= t3.get_untrained_net()\n",
    "init_net= deepcopy(net)\n",
    "import os\n",
    "if os.path.exists(model_dir3+'mnist_fc_trained_nn.pth'):\n",
    "    checkpoint = torch.load(model_dir3+'mnist_fc_trained_nn.pth', map_location=torch.device(device))\n",
    "    net.load_state_dict(checkpoint['state_dict'])  # Access the 'state_dict' within the loaded dictionary\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "else:\n",
    "    print(\"Train the network first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87258fbe-c4b8-49d9-8319-74b5a72a4584",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Verify NFA for FC layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "206e8d75-2d15-4529-9bdf-325264efc42b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (features): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (1): Nonlinearity()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (3): Nonlinearity()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=10, bias=False)\n",
      "  )\n",
      ")\n",
      "Correlation between Initial and Trained CNFM:  tensor(0.4899, device='cuda:0')\n",
      "Computing GOP for sample 0 out of 50\n",
      "Computing GOP for sample 1 out of 50\n",
      "Computing GOP for sample 2 out of 50\n",
      "Computing GOP for sample 3 out of 50\n",
      "Computing GOP for sample 4 out of 50\n",
      "Computing GOP for sample 5 out of 50\n",
      "Computing GOP for sample 6 out of 50\n",
      "Computing GOP for sample 7 out of 50\n",
      "Computing GOP for sample 8 out of 50\n",
      "Computing GOP for sample 9 out of 50\n",
      "Computing GOP for sample 10 out of 50\n",
      "Computing GOP for sample 11 out of 50\n",
      "Computing GOP for sample 12 out of 50\n",
      "Computing GOP for sample 13 out of 50\n",
      "Computing GOP for sample 14 out of 50\n",
      "Computing GOP for sample 15 out of 50\n",
      "Computing GOP for sample 16 out of 50\n",
      "Computing GOP for sample 17 out of 50\n",
      "Computing GOP for sample 18 out of 50\n",
      "Computing GOP for sample 19 out of 50\n",
      "Computing GOP for sample 20 out of 50\n",
      "Computing GOP for sample 21 out of 50\n",
      "Computing GOP for sample 22 out of 50\n",
      "Computing GOP for sample 23 out of 50\n",
      "Computing GOP for sample 24 out of 50\n",
      "Computing GOP for sample 25 out of 50\n",
      "Computing GOP for sample 26 out of 50\n",
      "Computing GOP for sample 27 out of 50\n",
      "Computing GOP for sample 28 out of 50\n",
      "Computing GOP for sample 29 out of 50\n",
      "Computing GOP for sample 30 out of 50\n",
      "Computing GOP for sample 31 out of 50\n",
      "Computing GOP for sample 32 out of 50\n",
      "Computing GOP for sample 33 out of 50\n",
      "Computing GOP for sample 34 out of 50\n",
      "Computing GOP for sample 35 out of 50\n",
      "Computing GOP for sample 36 out of 50\n",
      "Computing GOP for sample 37 out of 50\n",
      "Computing GOP for sample 38 out of 50\n",
      "Computing GOP for sample 39 out of 50\n",
      "Computing GOP for sample 40 out of 50\n",
      "Computing GOP for sample 41 out of 50\n",
      "Computing GOP for sample 42 out of 50\n",
      "Computing GOP for sample 43 out of 50\n",
      "Computing GOP for sample 44 out of 50\n",
      "Computing GOP for sample 45 out of 50\n",
      "Computing GOP for sample 46 out of 50\n",
      "Computing GOP for sample 47 out of 50\n",
      "Computing GOP for sample 48 out of 50\n",
      "Computing GOP for sample 49 out of 50\n",
      "Computing GOP for sample 50 out of 50\n",
      "Shape of grad matrix torch.Size([1024, 1024])\n",
      "Correlation between Trained CNFM and AGOP:  tensor(0.5470, device='cuda:0')\n",
      "Final:  tensor(0.4899, device='cuda:0') tensor(0.5470, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1521,  0.0036,  0.0123,  ..., -0.0081, -0.0019,  0.0023],\n",
       "        [ 0.0036,  0.1669, -0.0310,  ...,  0.0210, -0.0045,  0.0172],\n",
       "        [ 0.0123, -0.0310,  0.1918,  ..., -0.0199, -0.0021, -0.0093],\n",
       "        ...,\n",
       "        [-0.0081,  0.0210, -0.0199,  ...,  0.1433,  0.0097,  0.0215],\n",
       "        [-0.0019, -0.0045, -0.0021,  ...,  0.0097,  0.1614,  0.0088],\n",
       "        [ 0.0023,  0.0172, -0.0093,  ...,  0.0215,  0.0088,  0.1550]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(net)\n",
    "af.verify_NFA(net, init_net, trainloader, max_batch= 50, classes=10, chunk_idx=1, layer_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cdfd390-d9fa-4279-984b-a70d77281a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1024])\n",
      "Correlation between Initial and Trained CNFM:  tensor(0.4899, device='cuda:0', dtype=torch.float64)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (1): Nonlinearity()\n",
      ")\n",
      "Computing Jacobian for batch:  0 50\n",
      "Computing Jacobian for batch:  1 50\n",
      "Computing Jacobian for batch:  2 50\n",
      "Computing Jacobian for batch:  3 50\n",
      "Computing Jacobian for batch:  4 50\n",
      "Computing Jacobian for batch:  5 50\n",
      "Computing Jacobian for batch:  6 50\n",
      "Computing Jacobian for batch:  7 50\n",
      "Computing Jacobian for batch:  8 50\n",
      "Computing Jacobian for batch:  9 50\n",
      "Computing Jacobian for batch:  10 50\n",
      "Computing Jacobian for batch:  11 50\n",
      "torch.Size([9600, 10, 1024])\n",
      "0 12\n",
      "1 12\n",
      "2 12\n",
      "3 12\n",
      "4 12\n",
      "5 12\n",
      "6 12\n",
      "7 12\n",
      "8 12\n",
      "9 12\n",
      "10 12\n",
      "11 12\n",
      "Computing Jacobian for batch:  0 50\n",
      "Computing Jacobian for batch:  1 50\n",
      "Computing Jacobian for batch:  2 50\n",
      "Computing Jacobian for batch:  3 50\n",
      "Computing Jacobian for batch:  4 50\n",
      "Computing Jacobian for batch:  5 50\n",
      "Computing Jacobian for batch:  6 50\n",
      "Computing Jacobian for batch:  7 50\n",
      "Computing Jacobian for batch:  8 50\n",
      "Computing Jacobian for batch:  9 50\n",
      "Computing Jacobian for batch:  10 50\n",
      "Computing Jacobian for batch:  11 50\n",
      "torch.Size([9600, 10, 1024])\n",
      "0 12\n",
      "1 12\n",
      "2 12\n",
      "3 12\n",
      "4 12\n",
      "5 12\n",
      "6 12\n",
      "7 12\n",
      "8 12\n",
      "9 12\n",
      "10 12\n",
      "11 12\n",
      "Shape of grad matrix torch.Size([1024, 1024])\n",
      "Full Matrix Correlation Centered:  tensor(0.7705, device='cuda:0', dtype=torch.float64)\n",
      "Full Matrix Correlation Uncentered:  tensor(0.5471, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "af1.verify_NFA(net, init_net, trainloader, batch_size= 800, cutoff=10, chunk_idx=1, layer_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0696c3cb-e6fa-4e9f-b222-f1fa2845af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: How to meaningfully visualise? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb970f08-c3a0-405d-8ac3-f3311e2f88f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# RFM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fdd49e-4722-4f2d-887e-df6ad9b1c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Warning: This is an extremely cpu intensive process since it uses solve function from linalg \n",
    "The rfm.py from utils is equipped with more memory efficient solvers. \n",
    "'''\n",
    "\n",
    "rfm.rfm(trainloader, valloader, testloader, name=name,\n",
    "            batch_size=10, iters=1, reg=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398c7f3-04d0-4d0e-a420-df7ce183c68a",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f2150c4-24f4-4514-a748-18d616c91dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "#from numpy.linalg import solve\n",
    "from torch.linalg import solve\n",
    "import experiments.rfm_mnist.kernels as kernels #Should be called from utils\n",
    "from tqdm import tqdm\n",
    "import hickle\n",
    "import torch.distributions as distributions\n",
    "\n",
    "def laplace_kernel_M(pair1, pair2, bandwidth, M):\n",
    "    return kernels.laplacian_M(pair1, pair2, bandwidth, M)\n",
    "\n",
    "\n",
    "def get_grads(X, sol, L, P, batch_size=2):\n",
    "    torch.cuda.empty_cache()\n",
    "    M = 0.\n",
    "\n",
    "    num_samples = 10000\n",
    "    #indices = np.random.randint(len(X), size=num_samples)\n",
    "    indices = torch.randint(0, len(X), (num_samples,),device=device)\n",
    "\n",
    "    if len(X) > len(indices):\n",
    "        x = X[indices, :]\n",
    "    else:\n",
    "        x = X\n",
    "\n",
    "    K = laplace_kernel_M(X, x, L, P)\n",
    "\n",
    "    dist = kernels.euclidean_distances_M(X, x, P, squared=False)\n",
    "    \n",
    "    dist = torch.where(dist < 1e-10, torch.zeros(1,device=device).float(), dist)\n",
    "\n",
    "    K = K/dist\n",
    "    K[K == float(\"Inf\")] = 0.\n",
    "\n",
    "    #a1 = torch.from_numpy(sol.T).float()\n",
    "    a1= sol.T\n",
    "    n, d = X.shape\n",
    "    n, c = a1.shape\n",
    "    m, d = x.shape\n",
    "\n",
    "    a1 = a1.reshape(n, c, 1)\n",
    "    X1 = (X @ P).reshape(n, 1, d)\n",
    "    step1 = a1 @ X1\n",
    "    del a1, X1\n",
    "    step1 = step1.reshape(-1, c*d)\n",
    "\n",
    "    step2 = K.T @ step1\n",
    "    del step1\n",
    "\n",
    "    step2 = step2.reshape(-1, c, d)\n",
    "\n",
    "    #a2 = torch.from_numpy(sol).float()\n",
    "    a2 = sol\n",
    "    step3 = (a2 @ K).T\n",
    "\n",
    "    del K, a2\n",
    "\n",
    "    step3 = step3.reshape(m, c, 1)\n",
    "    x1 = (x @ P).reshape(m, 1, d)\n",
    "    step3 = step3 @ x1\n",
    "\n",
    "    G = (step2 - step3) * -1/L\n",
    "\n",
    "    M = 0.\n",
    "\n",
    "    bs = batch_size\n",
    "    batches = torch.split(G, bs)\n",
    "    for i in tqdm(range(len(batches))):\n",
    "        grad = batches[i].cuda()\n",
    "        gradT = torch.transpose(grad, 1, 2)\n",
    "        M += torch.sum(gradT @ grad, dim=0).cpu()\n",
    "        del grad, gradT\n",
    "    torch.cuda.empty_cache()\n",
    "    M /= len(G)\n",
    "    #M = M\n",
    "\n",
    "    return M\n",
    "\n",
    "def sample_normal(covariance_matrix_M, num_rows_k, matrix_dim_t):\n",
    "   \n",
    "    # Create a mean vector of zeros, compatible with the dimensions of M\n",
    "    dtype = covariance_matrix_M.dtype\n",
    "    mean_vector = torch.zeros(matrix_dim_t, dtype=dtype, device=device)\n",
    "    mean_vector.to(device)\n",
    "    #covariance_matrix_M.to(device)\n",
    "    # Create a multivariate normal distribution object\n",
    "    # torch.distributions.MultivariateNormal expects a covariance_matrix.\n",
    "    # It internally checks for positive definiteness.\n",
    "    try:\n",
    "        mvn = distributions.MultivariateNormal(loc=mean_vector, covariance_matrix=covariance_matrix_M)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating MultivariateNormal distribution: {e}\")\n",
    "        print(\"Please ensure your covariance_matrix_M is symmetric positive definite.\")\n",
    "        # Attempt to make it PSD for robustness in example, by adding a small diagonal perturbation\n",
    "        # In a real scenario, you'd ensure your input M is correctly formed.\n",
    "        min_eigval = torch.min(torch.linalg.eigvalsh(covariance_matrix_M)).item()\n",
    "        if min_eigval <= 0:\n",
    "            print(f\"Warning: Covariance matrix has non-positive eigenvalue ({min_eigval:.2e}). Adding jitter.\")\n",
    "            covariance_matrix_M = covariance_matrix_M + torch.eye(matrix_dim_t, device=device, dtype=dtype) * (abs(min_eigval) + 1e-6)\n",
    "            mvn = distributions.MultivariateNormal(loc=mean_vector, covariance_matrix=covariance_matrix_M)\n",
    "\n",
    "\n",
    "    # Sample num_rows_k times from the distribution\n",
    "    # The .sample() method will return a tensor of shape (num_rows_k, matrix_dim_t)\n",
    "    sampled_matrix = mvn.sample((num_rows_k,))\n",
    "\n",
    "    return sampled_matrix\n",
    "\n",
    "def rfm(train_loader, val_loader, test_loader,\n",
    "        iters=3, name=None, batch_size=2, reg=1e-3,\n",
    "        train_acc=True):\n",
    "\n",
    "    L = 10\n",
    "\n",
    "    X_train, y_train = get_data(train_loader)\n",
    "    X_val, y_val = get_data(val_loader)\n",
    "    X_test, y_test = get_data(test_loader)\n",
    "    \n",
    "    # Trimming: Ideally should be avoided\n",
    "    X_train, y_train = X_train[:30000], y_train[:30000]\n",
    "    X_val, y_val = X_val[:10000], y_val[:10000]\n",
    "    X_test, y_test = X_test[:10000], y_test[:10000]\n",
    "    \n",
    "    n, d = X_train.shape\n",
    "    \n",
    "    M = torch.eye(d, dtype=torch.float32)\n",
    "    \n",
    "    '''\n",
    "    X_train= X_train.to(device)\n",
    "    y_train= y_train.to(device)\n",
    "    #X_val.to(device)\n",
    "    #y_val.to(device)\n",
    "    x_test= X_test.to(device)\n",
    "    y_test= y_test.to(device)\n",
    "    '''\n",
    "    for i in range(iters):\n",
    "        print(i)\n",
    "        torch.cuda.empty_cache()\n",
    "        K_train = laplace_kernel_M(X_train.to(device), X_train.to(device), L, M.to(device))\n",
    "        #K_train.to(device)\n",
    "        source = (K_train + reg * torch.eye(len(K_train), device=device)).float()\n",
    "\n",
    "        \n",
    "        sol = solve(source, y_train.to(device)).T\n",
    "        sol = sol.to(device)\n",
    "        if train_acc:\n",
    "            y_pred = (sol @ K_train).T\n",
    "            #y_pred = torch.from_numpy(preds)\n",
    "            preds = torch.argmax(y_pred, dim=-1)\n",
    "            labels = torch.argmax(y_train, dim=-1)\n",
    "            count = torch.sum(labels.to(device) == preds.to(device))\n",
    "            print(\"Round \" + str(i) + \" Train Acc: \", count / len(labels))\n",
    "\n",
    "        K_test = laplace_kernel_M(X_train.to(device), X_test.to(device), L, M.to(device))\n",
    "        y_pred = (sol @ K_test).T\n",
    "        #print(\"Round \" + str(i) + \" MSE: \", torch.mean(torch.square(preds - y_test.numpy())))\n",
    "        print(\"Round \" + str(i) + \" MSE: \", F.mse_loss(y_pred.to(device), y_test.to(device)))\n",
    "        #y_pred = torch.from_numpy(preds)     \n",
    "        preds = torch.argmax(y_pred, dim=-1)\n",
    "        preds = preds.to(device)\n",
    "        labels = torch.argmax(y_test, dim=-1)\n",
    "        labels = labels.to(device)\n",
    "        count = torch.sum(labels == preds)\n",
    "        print(\"Round \" + str(i) + \" Acc: \", count / len(labels))\n",
    "\n",
    "        M  = get_grads(X_train.to(device), sol.to(device), L, M.to(device), batch_size=batch_size)\n",
    "        print(M.shape)\n",
    "        if name is not None:\n",
    "            hickle.dump(M, 'M_' + name + '_' + str(i) + '.h')\n",
    "    W = sample_normal(M.to(device), 1024,3072)\n",
    "    print(W)\n",
    "    print(W.shape)\n",
    "    print(net.features[0].weight)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    K_train = laplace_kernel_M(X_train, X_train, L, torch.from_numpy(M)).numpy()\n",
    "    sol = solve(K_train + reg * np.eye(len(K_train)), y_train).T\n",
    "    K_test = laplace_kernel_M(X_train, X_test, L, torch.from_numpy(M)).numpy()\n",
    "    preds = (sol @ K_test).T\n",
    "    mse = np.mean(np.square(preds - y_test.numpy()))\n",
    "    print(\"Final MSE: \", mse)\n",
    "    y_pred = torch.from_numpy(preds)\n",
    "    preds = torch.argmax(y_pred, dim=-1)\n",
    "    labels = torch.argmax(y_test, dim=-1)\n",
    "    count = torch.sum(labels == preds).numpy()\n",
    "    print(\" Final Acc: \", count / len(labels))\n",
    "    '''\n",
    "    #return mse\n",
    "\n",
    "\n",
    "def get_data(loader):\n",
    "    X = []\n",
    "    y = []\n",
    "    for idx, batch in enumerate(loader):\n",
    "        inputs, labels = batch\n",
    "        X.append(inputs)\n",
    "        y.append(labels)\n",
    "    return torch.cat(X, dim=0), torch.cat(y, dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8234842-3ac3-4786-a6cf-73c694431976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.35 GiB. GPU 0 has a total capacity of 15.48 GiB of which 3.20 GiB is free. Including non-PyTorch memory, this process has 12.27 GiB memory in use. Of the allocated memory 11.53 GiB is allocated by PyTorch, and 413.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrfm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfc_rfm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mrfm\u001b[39m\u001b[34m(train_loader, val_loader, test_loader, iters, name, batch_size, reg, train_acc)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[32m    139\u001b[39m torch.cuda.empty_cache()\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m K_train = \u001b[43mlaplace_kernel_M\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m#K_train.to(device)\u001b[39;00m\n\u001b[32m    142\u001b[39m source = (K_train + reg * torch.eye(\u001b[38;5;28mlen\u001b[39m(K_train), device=device)).float()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mlaplace_kernel_M\u001b[39m\u001b[34m(pair1, pair2, bandwidth, M)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlaplace_kernel_M\u001b[39m(pair1, pair2, bandwidth, M):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkernels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlaplacian_M\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbandwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DLR/experiments/rfm_mnist/kernels.py:99\u001b[39m, in \u001b[36mlaplacian_M\u001b[39m\u001b[34m(samples, centers, bandwidth, M)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlaplacian_M\u001b[39m(samples, centers, bandwidth, M):\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m bandwidth > \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     kernel_mat = \u001b[43meuclidean_distances_M\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m     kernel_mat.clamp_(\u001b[38;5;28mmin\u001b[39m=\u001b[32m0\u001b[39m)\n\u001b[32m    101\u001b[39m     gamma = \u001b[32m1.\u001b[39m / bandwidth\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DLR/experiments/rfm_mnist/kernels.py:41\u001b[39m, in \u001b[36meuclidean_distances_M\u001b[39m\u001b[34m(samples, centers, M, squared)\u001b[39m\n\u001b[32m     37\u001b[39m     centers_norm = torch.sum(centers_norm, dim=\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     39\u001b[39m centers_norm = torch.reshape(centers_norm, (\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m distances = \u001b[43msamples\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m distances.mul_(-\u001b[32m2\u001b[39m)\n\u001b[32m     43\u001b[39m distances.add_(samples_norm)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 3.35 GiB. GPU 0 has a total capacity of 15.48 GiB of which 3.20 GiB is free. Including non-PyTorch memory, this process has 12.27 GiB memory in use. Of the allocated memory 11.53 GiB is allocated by PyTorch, and 413.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "rfm(trainloader, valloader, testloader, name='fc_rfm',\n",
    "            batch_size=10, iters=10, reg=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0aaba271-fa28-45eb-8d17-6496e380466c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 3072])\n",
      "Parameter containing:\n",
      "tensor([[-0.0164,  0.0110,  0.0087,  ...,  0.0010,  0.0019,  0.0024],\n",
      "        [-0.0033, -0.0127,  0.0155,  ...,  0.0079, -0.0117, -0.0122],\n",
      "        [-0.0145, -0.0007,  0.0079,  ..., -0.0125,  0.0094, -0.0095],\n",
      "        ...,\n",
      "        [ 0.0080,  0.0042, -0.0011,  ..., -0.0018,  0.0047, -0.0160],\n",
      "        [-0.0113,  0.0027,  0.0152,  ...,  0.0084, -0.0130,  0.0136],\n",
      "        [-0.0171, -0.0143,  0.0057,  ..., -0.0158,  0.0157,  0.0007]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(net.features[0].weight.shape)\n",
    "print(net.features[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "28cc78e1-b6fe-49bc-b957-10c67afd2503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (features): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (1): Nonlinearity()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (3): Nonlinearity()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=10, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f329e-fd21-4f0f-b069-94330314d27b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
