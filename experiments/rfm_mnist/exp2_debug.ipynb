{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132b5a3f-434a-480b-b700-57080e223ebe",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860fdfce-6606-49d9-8a09-30882b61807b",
   "metadata": {},
   "source": [
    "This experiment checks the following for a simple convnet on MNIST.\n",
    "1. Verify Agop and NFM relations for the conv layers\n",
    "2. Run RFM to construct similar matrices as the above.(TBD)\n",
    "\n",
    "The model is taken from MNIST/model2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20927ac7-5a3a-47f7-b1c3-4ab1da39990d",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b908e4f-a112-4597-85f8-3bd750a99aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "parent_dir='C:\\\\Users\\\\garav\\\\AGOP\\\\DLR'\n",
    "model_dir= 'C:\\\\Users\\\\garav\\\\AGOP\\\\DLR\\\\trained_models\\\\MNIST\\\\model2\\\\nn_models\\\\'\n",
    "#parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2a63f9a-f5d9-452f-a906-f07545fd3d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Without the incoming socket you cannot receive events from the server or register event handlers to your Visdom client.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garav\\AGOP\\DLR\\experiments\\rfm_mnist\n",
      "C:\\Users\\garav\\AGOP\\DLR\n",
      "C:\\Users\\garav\\AGOP\\DLR\\trained_models\\MNIST\\model2\\nn_models\\\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from utils import agop_conv as agc\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import rfm\n",
    "import numpy as np\n",
    "from trained_models.MNIST.model2 import trainer as t\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.linalg import norm\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8f9d450-8db0-4963-9a9e-dfaff72b63f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed7a279d-7a07-49a0-85e6-615dfa1faf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ee3a4f9-1285-4aad-bfcc-55fd6623a60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "trainloader, valloader, testloader = t.get_loaders()\n",
    "net= t.get_untrained_net()\n",
    "init_net= deepcopy(net)\n",
    "import os\n",
    "if os.path.exists(model_dir+'mnist_conv_trained_nn.pth'):\n",
    "    checkpoint = torch.load(model_dir+'mnist_conv_trained_nn.pth', map_location=torch.device(device))\n",
    "    net.load_state_dict(checkpoint['state_dict'])  # Access the 'state_dict' within the loaded dictionary\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "else:\n",
    "    print(\"Train the network first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a974e0-9a54-4d0f-aafc-d04211dfdb7d",
   "metadata": {},
   "source": [
    "# Debug Task: Highlight the importance of patchconve and the need to sperate variables for Jacrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "02c8f4db-df8e-4504-8a06-29da71044e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between Initial and Trained CNFM:  tensor(0.1139, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Computing GOP for sample 0 out of 10\n",
      "Computing GOP for sample 1 out of 10\n",
      "Computing GOP for sample 2 out of 10\n",
      "Computing GOP for sample 3 out of 10\n",
      "Computing GOP for sample 4 out of 10\n",
      "Computing GOP for sample 5 out of 10\n",
      "Computing GOP for sample 6 out of 10\n",
      "Computing GOP for sample 7 out of 10\n",
      "Computing GOP for sample 8 out of 10\n",
      "Computing GOP for sample 9 out of 10\n",
      "Computing GOP for sample 10 out of 10\n",
      "Shpae after gradients:  torch.Size([288, 288])\n",
      "Correlation between Trained CNFM and AGOP:  tensor(0.9106, device='cuda:0')\n",
      "Final:  tensor(0.1139, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.9106, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Here layer_idx refers to layer_idx+1 th conv layer. \n",
    "G = agc.verify_NFA(net.to(device), init_net.to(device), trainloader, layer_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90b316c9-db51-49a5-8cf8-22d964ff5a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 3, 3])\n",
      "Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 0\n",
      "Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 1\n",
      "torch.Size([64, 14, 14, 32, 3, 3])\n",
      "torch.Size([64, 32, 3, 3])\n",
      "Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 0\n",
      "Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 1\n",
      "torch.Size([64, 14, 14, 32, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "agc.vis_transform_image(net, imgs[0], None, layer_idx=1)\n",
    "agc.vis_transform_image(net, imgs[0], G, layer_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2fa6cb-525b-4388-87be-3d271a912721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62deddfd-2ffd-4da7-86ec-86e6b572ee92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Without the incoming socket you cannot receive events from the server or register event handlers to your Visdom client.\n"
     ]
    }
   ],
   "source": [
    "''' This module does the following\n",
    "1. Scan the network for conv layers\n",
    "2. For each conv layer compute W^TW of eq 3\n",
    "3. For each conv layer compute the AGOP(AJOP in case of multiple outputs)\n",
    "4. For each conv layer print the pearson correlation between 2 and 3\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "#from functorch import jacrev, vmap\n",
    "from torch.func import jacrev\n",
    "from torch.nn.functional import pad\n",
    "#import dataset\n",
    "from numpy.linalg import eig\n",
    "from copy import deepcopy\n",
    "from torch.linalg import norm, svd\n",
    "from torchvision import models\n",
    "import visdom\n",
    "from torch.linalg import norm, eig\n",
    "\n",
    "\n",
    "SEED = 2323\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "vis = visdom.Visdom('http://127.0.0.1', use_incoming_socket=False)\n",
    "vis.close(env='main')\n",
    "\n",
    "\n",
    "def unpatchify(patches, original_shape, patch_size, stride_size, padding=None, pad_type='zeros'):\n",
    "    \"\"\"\n",
    "    Reconstructs an image from its patches, reversing the operation of the patchify function.\n",
    "\n",
    "    Args:\n",
    "        patches: Tensor of shape (n, h_out, w_out, c, q1, q2), the output of patchify.\n",
    "        original_shape: The shape of the original image (n, c, h, w).\n",
    "        patch_size: Tuple (q1, q2), the size of each patch.\n",
    "        stride_size: Tuple (s1, s2), the stride used in patchify.\n",
    "        padding: Optional tuple (pad_1, pad_2), the padding used in patchify.\n",
    "        pad_type: String, the padding type used in patchify ('zeros' or 'circular').\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (n, c, h, w), the reconstructed image.\n",
    "    \"\"\"\n",
    "    n, c, h, w = original_shape\n",
    "    q1, q2 = patch_size\n",
    "    s1, s2 = stride_size\n",
    "    if padding is None:\n",
    "        pad_1 = (q1 - 1) // 2\n",
    "        pad_2 = (q2 - 1) // 2\n",
    "    else:\n",
    "        pad_1, pad_2 = padding\n",
    "\n",
    "    # Calculate output dimensions of the patches\n",
    "    h_out = (h + 2 * pad_1 - q1) // s1 + 1\n",
    "    w_out = (w + 2 * pad_2 - q2) // s2 + 1\n",
    "\n",
    "    # Check if the patches shape is correct\n",
    "    if patches.shape != (n, h_out, w_out, c, q1, q2):\n",
    "        # raise ValueError(f\"Input patches shape {patches.shape} does not match expected shape {(n, h_out, w_out, c, q1, q2)}\")\n",
    "        print(f\"Warning: Input patches shape {patches.shape} does not match expected shape {(n, h_out, w_out, c, q1, q2)}. Attempting to adjust.\")\n",
    "        if patches.shape[1] != h_out or patches.shape[2] != w_out:\n",
    "            raise ValueError(\"Cannot automatically adjust patches shape.  h_out or w_out mismatch is unrecoverable.\")\n",
    "\n",
    "\n",
    "    # Create an empty tensor for the reconstructed image with padded size\n",
    "    padded_h = h + 2 * pad_1\n",
    "    padded_w = w + 2 * pad_2\n",
    "    reconstructed_padded = torch.zeros((n,c, padded_h, padded_w), dtype=patches.dtype, device=patches.device)\n",
    "    print(reconstructed_padded.shape)\n",
    "    print(patches.shape)\n",
    "    # Create a weight tensor to count how many times each pixel is written to.\n",
    "    weight_map = torch.zeros((n, 1, padded_h, padded_w), dtype=patches.dtype, device=patches.device)\n",
    "\n",
    "    # Iterate over the patches and add them to the reconstructed image\n",
    "    for i in range(min(h_out, patches.shape[1])):\n",
    "        for j in range(min(w_out, patches.shape[2])):\n",
    "            # Add the patch values to the reconstructed image\n",
    "            reconstructed_padded[:,:, i * s1:i * s1 + q1, j * s2:j * s2 + q2] += patches[:, i, j]\n",
    "            weight_map[:,:, i * s1:i * s1 + q1, j * s2:j * s2 + q2] += 1\n",
    "\n",
    "    # Average the overlapping pixels\n",
    "    reconstructed_padded = reconstructed_padded / weight_map\n",
    "\n",
    "    # Remove padding\n",
    "    if pad_type == 'zeros':\n",
    "        reconstructed_image = reconstructed_padded[:,:, pad_1:pad_1 + h, pad_2:pad_2 + w]\n",
    "    elif pad_type == 'circular':\n",
    "        # For circular padding, we need to handle the circular shift\n",
    "        reconstructed_image = reconstructed_padded[:,:, pad_1:pad_1 + h, pad_2:pad_2 + w]\n",
    "        # Circular padding introduces overlap from the other side, which has been correctly averaged.\n",
    "    else:\n",
    "        reconstructed_image = reconstructed_padded[:,:, pad_1:pad_1 + h, pad_2:pad_2 + w]\n",
    "\n",
    "    return reconstructed_image\n",
    "\n",
    " \n",
    "\n",
    "def patchify(x, patch_size, stride_size, padding=None, pad_type='zeros'):\n",
    "    '''\n",
    "        Given an input image (n,chunk,c,h,w) generate (n,w_out,h_out,c,q,s) respecting stride,padding, \n",
    "        w_out is number of pathces along the width for the given stride after padding\n",
    "        h_out is number of pathces along the height for the given stride after padding\n",
    "        (q,s) is the kernel dimensions \n",
    "    '''\n",
    "    q1, q2 = patch_size\n",
    "    s1, s2 = stride_size\n",
    "    \n",
    "    if padding is None:\n",
    "        pad_1 = (q1-1)//2\n",
    "        pad_2 = (q2-1)//2\n",
    "    else:\n",
    "        pad_1, pad_2 = padding\n",
    "\n",
    "    pad_dims = (pad_2, pad_2, pad_1, pad_1)\n",
    "    if pad_type == 'zeros':\n",
    "        x = pad(x, pad_dims)\n",
    "    elif pad_type == 'circular':\n",
    "        x = pad(x, pad_dims, 'circular')\n",
    "    elif pad_type == 'dont':\n",
    "        x= x\n",
    "        \n",
    "    patches = x.unfold(3, q1, s1).unfold(4, q2, s2) #(n,chunk,c, h_out, w_out, q, s)\n",
    "    patches = patches.transpose(2, 4).transpose(2, 3) #(n,chunk, h_out,w_out,c,q,s) \n",
    "    return patches\n",
    "    \n",
    "def patchify1(x, patch_size, stride_size, padding=None, pad_type='zeros'):\n",
    "    '''\n",
    "        Given an input image (n,c,h,w) generate (n,w_out,h_out,c,q,s) respecting stride,padding, \n",
    "        w_out is number of pathces along the width for the given stride after padding\n",
    "        h_out is number of pathces along the height for the given stride after padding\n",
    "        (q,s) is the kernel dimensions \n",
    "    '''\n",
    "    q1, q2 = patch_size\n",
    "    s1, s2 = stride_size\n",
    "    print(\"Image Shape\",x.shape)\n",
    "    if padding is None:\n",
    "        pad_1 = (q1-1)//2\n",
    "        pad_2 = (q2-1)//2\n",
    "    else:\n",
    "        pad_1, pad_2 = padding\n",
    "\n",
    "    pad_dims = (pad_2, pad_2, pad_1, pad_1)\n",
    "    if pad_type == 'zeros':\n",
    "        x = pad(x, pad_dims)\n",
    "    elif pad_type == 'circular':\n",
    "        x = pad(x, pad_dims, 'circular')\n",
    "        \n",
    "    patches = x.unfold(2, q1, s1).unfold(3, q2, s2) #(n, c, h_out, w_out, q, s)\n",
    "    print(\"Image Shape1\",patches.shape)\n",
    "    patches = patches.transpose(1, 3).transpose(1, 2) #(n,w_out,h_out,c,q,s) \n",
    "    print(\"Image Shape2\",patches.shape)\n",
    "    return patches\n",
    "    \n",
    "class PatchConvLayer(nn.Module):\n",
    "    def __init__(self, conv_layer):\n",
    "        super().__init__()\n",
    "        self.layer = conv_layer #(k,c,q,s)\n",
    "\n",
    "    def forward(self, patches):\n",
    "        #Todo: 1. Check why the format is nwhcqr when the patches after patchify is nhwcqr.\n",
    "        #      2. Why does this output n,k,w,h when the standard format is n,k,h,w\n",
    "        out = torch.einsum('nwhcqr, kcqr -> nwhk', patches, self.layer.weight)\n",
    "        n, w, h, k = out.shape\n",
    "        out = out.transpose(1, 3).transpose(2, 3) #Should be (n,k,h_out,w_out) even though w and h are swapped\n",
    "        return out\n",
    "\n",
    "def get_jacobian(net, imgs, original_shape, kernel,stride,padding, c_idx=0, chunk=100):\n",
    "    with torch.no_grad():\n",
    "        def single_net(x):\n",
    "            # x is (h_out,w_out,c,q,s)\n",
    "            #return net(x.unsqueeze(0))[:,c_idx*chunk:(c_idx+1)*chunk].squeeze(0)\n",
    "            print(\"x shape\",original_shape)\n",
    "            net.eval()\n",
    "            #x= x.unsqueeze(0)\n",
    "            reconstructed_shape = (1,original_shape[1], original_shape[2], original_shape[3])\n",
    "            x= unpatchify(x.unsqueeze(0),reconstructed_shape,kernel,stride,padding)\n",
    "            #return x\n",
    "            return net(x)[:,c_idx*chunk:(c_idx+1)*chunk].squeeze(0)\n",
    "        # Parallelize across the images.\n",
    "        #print(imgs.shape)\n",
    "        #return torch.vmap(jacrev(single_net))(imgs) #(n, chunk, h_out, w_out, c, q, s)\n",
    "        jacobian_results = []\n",
    "        # imgs here is the output of patchify1 from get_grads, shape (n_batch, h_out, w_out, c, q, s)\n",
    "        n_batch = imgs.shape[0]\n",
    "        \n",
    "        for i in range(20):\n",
    "            # Apply jacrev to each individual set of patches\n",
    "            # imgs[i] has shape (h_out, w_out, c, q, s)\n",
    "            single_jacobian = jacrev(single_net)(imgs[i])\n",
    "            jacobian_results.append(single_jacobian)\n",
    "\n",
    "        # Concatenate the results along the batch dimension\n",
    "        # The output shape will be (n_batch, output_chunk_size, h_out, w_out, c, q, s)\n",
    "        return torch.stack(jacobian_results, dim=0)\n",
    "        \n",
    "\n",
    "\n",
    "def egop(model, imgs, original_shape, kernel=(3,3), padding=(1,1),\n",
    "              stride=(1,1)):\n",
    "    ajop = 0\n",
    "    c = 10\n",
    "    chunk_idxs = 1\n",
    "    #Chunking is done to compute jacobian as chunks. This saves memory\n",
    "    #TODO: chunk should be passed as argument\n",
    "    chunk = c // chunk_idxs\n",
    "    for i in range(chunk_idxs):\n",
    "        J = get_jacobian(model, imgs, original_shape, kernel,stride,padding, c_idx=i, chunk=chunk)\n",
    "        print(J.shape)\n",
    "        #patches = patchify(J, kernel, stride, padding)#(n,w_out,h_out,c,q,s)\n",
    "        #patches = patches.cuda()\n",
    "        #print(\"patch shape\",patches.shape)\n",
    "        n, c, w, h, _, _, _ = J.shape\n",
    "        patches = J.transpose(1, 3).transpose(1, 2) #(n, h_out, w_out, chunk, c, q, s)\n",
    "        grads = patches.reshape(n*w*h, c, -1) #(n*w_out*h_out, chunk, c*q*s)\n",
    "        print(\"Grads:\",grads.shape)\n",
    "        #Clarify: Where is mean taken\n",
    "        ajop += torch.einsum('ncd, ncD -> dD', grads, grads) #(c*q*s,c*q*s)\n",
    "    return ajop\n",
    "\n",
    "\n",
    "def load_nn(net, init_net, layer_idx=0):\n",
    "   \n",
    "    count = 0\n",
    "    \n",
    "    # Get the layer_idx+1 th conv layer\n",
    "    for idx, m in enumerate(net.features):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            count += 1\n",
    "        if count-1 == layer_idx:\n",
    "            l_idx = idx\n",
    "            break\n",
    "    \n",
    "    patchnet = deepcopy(net)\n",
    "    #layer = PatchConvLayer(net.features[l_idx])\n",
    "    #layer = net.features[l_idx]\n",
    "    # Extract all the meta info of the current conv layer.\n",
    "    (q, s) = net.features[l_idx].kernel_size\n",
    "    (pad1, pad2) = net.features[l_idx].padding\n",
    "    \n",
    "    # Truncate all layers before l_idx and wrap the current conv layer as a PatchConvLayer class.\n",
    "    (s1, s2) = net.features[l_idx].stride\n",
    "    patchnet.features = net.features[l_idx:]\n",
    "    #patchnet.features[0] = layer\n",
    "    patchnet.features[0].bias= None\n",
    "    M = net.features[l_idx].weight\n",
    "    _, ki, q, s = M.shape\n",
    "    M0 = init_net.features[l_idx].weight\n",
    "    M = M.reshape(-1, ki*q*s)\n",
    "    M0 = M0.reshape(-1, ki*q*s)\n",
    "    M = torch.einsum('nd, nD -> dD', M, M)\n",
    "    M0 = torch.einsum('nd, nD -> dD', M0, M0)\n",
    "    ''' \n",
    "        count = -1\n",
    "        for idx, p in enumerate(net.parameters()):\n",
    "            \n",
    "            # This logic of identifying layer_idx+1th parameters is not generic. It will fail if Batchnorm2d is present\n",
    "            # Why not get weights directly from the layer object?\n",
    "            if len(p.shape) > 1:\n",
    "                count += 1\n",
    "            if count == layer_idx:\n",
    "                M = p.data #(k,c,q,s)\n",
    "                _, ki, q, s = M.shape\n",
    "                \n",
    "                # Build W which is a (k, c*q*s) matrix\n",
    "                M = M.reshape(-1, ki*q*s)\n",
    "                \n",
    "                # Compute WtW which is (c*q*s,c*q*s) matrix\n",
    "                M = torch.einsum('nd, nD -> dD', M, M)\n",
    "                \n",
    "                # Build W0 from the untrained net which is a (k, c*q*s) matrix\n",
    "                M0 = [p for p in init_net.parameters()][idx]          \n",
    "                M0 = M0.reshape(-1, ki*q*s)\n",
    "                \n",
    "                # Compute W0tW0 which is (c*q*s,c*q*s) matrix\n",
    "                M0 = torch.einsum('nd, nD -> dD', M0, M0)\n",
    "                break\n",
    "              '''\n",
    "    return net, patchnet, M, M0, l_idx, [(q, s), (pad1,pad2), (s1,s2)]\n",
    "\n",
    "\n",
    "def get_grads(net, patchnet, trainloader,\n",
    "              kernel=(3,3), padding=(1,1),\n",
    "              stride=(1,1), layer_idx=0):\n",
    "    net.eval()\n",
    "    net.cuda()\n",
    "    patchnet.eval()\n",
    "    patchnet.cuda()\n",
    "    M = 0\n",
    "    q, s = kernel\n",
    "    pad1, pad2 = padding\n",
    "    s1, s2 = stride\n",
    "\n",
    "    # Num images for taking AGOP (Can be small for early layers)\n",
    "    MAX_NUM_IMGS = 10\n",
    "\n",
    "    for idx, batch in enumerate(trainloader):\n",
    "        print(\"Computing GOP for sample \" + str(idx) + \\\n",
    "              \" out of \" + str(MAX_NUM_IMGS))\n",
    "        imgs, _ = batch\n",
    "        with torch.no_grad():\n",
    "            imgs = imgs.cuda()        \n",
    "            # Run the first half of the network wrt to the current layer \n",
    "            imgs = net.features[:layer_idx](imgs) #(n,c,h,w)\n",
    "            imgs = pad(imgs, padding)\n",
    "        patches = patchify1(imgs, (q, s), (s1,s2), padding=(0,0))#(n,w_out,h_out,c,q,s)\n",
    "        patches = patches.cuda()\n",
    "        M += egop(patchnet, patches, imgs.shape, kernel=(q,s),stride=(s1,s2), padding=(pad1,pad2)).cpu()\n",
    "        del imgs\n",
    "        torch.cuda.empty_cache()\n",
    "        if idx >= MAX_NUM_IMGS:\n",
    "            break\n",
    "    net.cpu()\n",
    "    patchnet.cpu()\n",
    "    return M\n",
    "\n",
    "\n",
    "def min_max(M):\n",
    "    return (M - M.min()) / (M.max() - M.min())\n",
    "\n",
    "\n",
    "def correlation(M1, M2):\n",
    "    M1 -= M1.mean()\n",
    "    M2 -= M2.mean()\n",
    "\n",
    "    norm1 = norm(M1.flatten())\n",
    "    norm2 = norm(M2.flatten())\n",
    "\n",
    "    return torch.sum(M1.cuda() * M2.cuda()) / (norm1 * norm2)\n",
    "\n",
    "\n",
    "def verify_NFA(net, init_net, trainloader, layer_idx=0):\n",
    "\n",
    "\n",
    "    net, patchnet, M, M0, l_idx, conv_vals = load_nn(net,\n",
    "                                                     init_net,\n",
    "                                                     layer_idx=layer_idx)\n",
    "    (q, s), (pad1, pad2), (s1, s2) = conv_vals\n",
    "\n",
    "    i_val = correlation(M0, M)\n",
    "    print(\"Correlation between Initial and Trained CNFM: \", i_val)\n",
    "\n",
    "    G = get_grads(net, patchnet, trainloader,\n",
    "                  kernel=(q, s),\n",
    "                  padding=(pad1, pad2),\n",
    "                  stride=(s1, s2),\n",
    "                  layer_idx=l_idx)\n",
    "    print(\"Shpae after gradients: \", G.shape)\n",
    "    G = sqrt(G)\n",
    "    Gop = G.clone()\n",
    "    r_val = correlation(M, G)\n",
    "    print(\"Correlation between Trained CNFM and AGOP: \", r_val)\n",
    "    print(\"Final: \", i_val, r_val)\n",
    "    return Gop \n",
    "    #return i_val.data.numpy(), r_val.data.numpy()\n",
    "\n",
    "def vis_transform_image(net, img, G, layer_idx=0):\n",
    "\n",
    "    count = -1\n",
    "    \n",
    "    # Computes WtW for the weights(ignoring its bias) of layer_idx+1 the conv layer\n",
    "    for idx, p in enumerate(net.parameters()):\n",
    "        if len(p.shape) > 1:\n",
    "            count += 1\n",
    "        if count == layer_idx:\n",
    "            M = p.data\n",
    "            print(M.shape)\n",
    "            _, ki, q, s = M.shape\n",
    "\n",
    "            M = M.reshape(-1, ki*q*s)\n",
    "            M = torch.einsum('nd, nD -> dD', M, M)\n",
    "            break\n",
    "\n",
    "    count = 0\n",
    "    l_idx = None\n",
    "    \n",
    "    # Get the layer_idx+1 conv layer \n",
    "    for idx, m in enumerate(net.features):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            print(m, count)\n",
    "            count += 1\n",
    "\n",
    "        if count-1 == layer_idx:\n",
    "            l_idx = idx\n",
    "            break\n",
    "\n",
    "    net.eval()\n",
    "    net.cuda()\n",
    "    img = img.cuda()\n",
    "    img = net.features[:l_idx](img).cpu()\n",
    "    net.cpu()\n",
    "    \n",
    "    # If G is given which is expected to be the AGOP of layer_idx+1 conv layer then that is used.\n",
    "    if G is not None:\n",
    "        M = G\n",
    "\n",
    "    patches = patchify(img, (q, s), (1, 1))\n",
    "    \n",
    "    print(patches.shape)\n",
    "    # Patches should will be of the shape (n,w,h,c,q,s) not (n,w,h,q,s,c)\n",
    "    n, w, h, q, s, c = patches.shape\n",
    "    # Vectorize each patch\n",
    "    patches = patches.reshape(n, w, h, q*s*c)\n",
    "    # Apply either WtW or AGOP of the layer_idx+1 conv to each patch. D is c*q*s vector\n",
    "    M_patch = torch.einsum('nwhd, dD -> nwhD', patches, M) #(n,w,h,c*q*s)\n",
    "    \n",
    "    M_patch = norm(M_patch, dim=-1) #(n,w,h)\n",
    "\n",
    "    vis.image(min_max(M_patch[0])) #(w,h) image.\n",
    "\n",
    "\n",
    "def sqrt(G):\n",
    "    U, s, Vt = svd(G)\n",
    "    s = torch.pow(s, 1./2)\n",
    "    G = U @ torch.diag(s) @ Vt\n",
    "    return G\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Adjust to index conv layers in VGGs\n",
    "    idxs = list(range(8))\n",
    "\n",
    "    fname = 'csv_logs/test.csv'\n",
    "    outf = open(fname, 'w')\n",
    "\n",
    "    net = models.vgg11(weights=\"DEFAULT\")\n",
    "    #init_net is used as a reference untrained network.\n",
    "    init_net = models.vgg11(weights=None)\n",
    "\n",
    "    # Modules is unused.\n",
    "    modules= list(net.children())[:-1]\n",
    "    modules += [nn.Flatten(), list(net.children())[-1]]\n",
    "\n",
    "    # Set path to imagenet data\n",
    "    path = None\n",
    "\n",
    "    trainloader, _ = dataset.get_imagenet(batch_size=2, path=path)\n",
    "\n",
    "    for idx in idxs:\n",
    "        i_val, r_val = verify_NFA(net, init_net, trainloader, layer_idx=idx)\n",
    "        print(\"Layer \" + str(idx+1) + ',' + str(i_val) + ',' + str(r_val), file=outf, flush=True)\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "36487360-17c8-4057-9f3d-bf7bc60a7078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between Initial and Trained CNFM:  tensor(0.1139, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Computing GOP for sample 0 out of 10\n",
      "Image Shape torch.Size([64, 32, 14, 14])\n",
      "Image Shape1 torch.Size([64, 32, 14, 14, 3, 3])\n",
      "Image Shape2 torch.Size([64, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "torch.Size([20, 10, 14, 14, 32, 3, 3])\n",
      "Grads: torch.Size([3920, 10, 288])\n",
      "Computing GOP for sample 1 out of 10\n",
      "Image Shape torch.Size([64, 32, 14, 14])\n",
      "Image Shape1 torch.Size([64, 32, 14, 14, 3, 3])\n",
      "Image Shape2 torch.Size([64, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "torch.Size([20, 10, 14, 14, 32, 3, 3])\n",
      "Grads: torch.Size([3920, 10, 288])\n",
      "Computing GOP for sample 2 out of 10\n",
      "Image Shape torch.Size([64, 32, 14, 14])\n",
      "Image Shape1 torch.Size([64, 32, 14, 14, 3, 3])\n",
      "Image Shape2 torch.Size([64, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "torch.Size([20, 10, 14, 14, 32, 3, 3])\n",
      "Grads: torch.Size([3920, 10, 288])\n",
      "Computing GOP for sample 3 out of 10\n",
      "Image Shape torch.Size([64, 32, 14, 14])\n",
      "Image Shape1 torch.Size([64, 32, 14, 14, 3, 3])\n",
      "Image Shape2 torch.Size([64, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "torch.Size([20, 10, 14, 14, 32, 3, 3])\n",
      "Grads: torch.Size([3920, 10, 288])\n",
      "Computing GOP for sample 4 out of 10\n",
      "Image Shape torch.Size([64, 32, 14, 14])\n",
      "Image Shape1 torch.Size([64, 32, 14, 14, 3, 3])\n",
      "Image Shape2 torch.Size([64, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "torch.Size([20, 10, 14, 14, 32, 3, 3])\n",
      "Grads: torch.Size([3920, 10, 288])\n",
      "Computing GOP for sample 5 out of 10\n",
      "Image Shape torch.Size([64, 32, 14, 14])\n",
      "Image Shape1 torch.Size([64, 32, 14, 14, 3, 3])\n",
      "Image Shape2 torch.Size([64, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "torch.Size([20, 10, 14, 14, 32, 3, 3])\n",
      "Grads: torch.Size([3920, 10, 288])\n",
      "Computing GOP for sample 6 out of 10\n",
      "Image Shape torch.Size([64, 32, 14, 14])\n",
      "Image Shape1 torch.Size([64, 32, 14, 14, 3, 3])\n",
      "Image Shape2 torch.Size([64, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "torch.Size([20, 10, 14, 14, 32, 3, 3])\n",
      "Grads: torch.Size([3920, 10, 288])\n",
      "Computing GOP for sample 7 out of 10\n",
      "Image Shape torch.Size([64, 32, 14, 14])\n",
      "Image Shape1 torch.Size([64, 32, 14, 14, 3, 3])\n",
      "Image Shape2 torch.Size([64, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "torch.Size([20, 10, 14, 14, 32, 3, 3])\n",
      "Grads: torch.Size([3920, 10, 288])\n",
      "Computing GOP for sample 8 out of 10\n",
      "Image Shape torch.Size([64, 32, 14, 14])\n",
      "Image Shape1 torch.Size([64, 32, 14, 14, 3, 3])\n",
      "Image Shape2 torch.Size([64, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "torch.Size([20, 10, 14, 14, 32, 3, 3])\n",
      "Grads: torch.Size([3920, 10, 288])\n",
      "Computing GOP for sample 9 out of 10\n",
      "Image Shape torch.Size([64, 32, 14, 14])\n",
      "Image Shape1 torch.Size([64, 32, 14, 14, 3, 3])\n",
      "Image Shape2 torch.Size([64, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "torch.Size([20, 10, 14, 14, 32, 3, 3])\n",
      "Grads: torch.Size([3920, 10, 288])\n",
      "Computing GOP for sample 10 out of 10\n",
      "Image Shape torch.Size([64, 32, 14, 14])\n",
      "Image Shape1 torch.Size([64, 32, 14, 14, 3, 3])\n",
      "Image Shape2 torch.Size([64, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "x shape torch.Size([64, 32, 14, 14])\n",
      "torch.Size([1, 32, 16, 16])\n",
      "torch.Size([1, 14, 14, 32, 3, 3])\n",
      "torch.Size([20, 10, 14, 14, 32, 3, 3])\n",
      "Grads: torch.Size([3920, 10, 288])\n",
      "Shpae after gradients:  torch.Size([288, 288])\n",
      "Correlation between Trained CNFM and AGOP:  tensor(0.6806, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Final:  tensor(0.1139, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.6806, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9666,  0.4389,  0.1349,  ...,  0.0247,  0.0260,  0.0213],\n",
       "        [ 0.4389,  1.8607,  0.4309,  ...,  0.0117,  0.0124,  0.0256],\n",
       "        [ 0.1349,  0.4309,  1.9371,  ..., -0.0134,  0.0101,  0.0143],\n",
       "        ...,\n",
       "        [ 0.0247,  0.0116, -0.0134,  ...,  1.3624,  0.1564, -0.0092],\n",
       "        [ 0.0260,  0.0124,  0.0101,  ...,  0.1564,  1.3575,  0.1567],\n",
       "        [ 0.0213,  0.0256,  0.0143,  ..., -0.0092,  0.1567,  1.4506]])"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_NFA(net.to(device), init_net.to(device), trainloader, layer_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8666fbb-7e50-4e44-a505-b980220a5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, c, h, w = 2, 1, 4, 5\n",
    "x_4d = torch.arange(n * c * h * w).reshape(n, c, h, w) + 1\n",
    "print(\"\\nOriginal 4D Tensor (Batch of Images):\\n\", x_4d)\n",
    "print(\"Shape:\", x_4d.shape)\n",
    "\n",
    "patch_size = (3, 3)\n",
    "stride = (2, 2)\n",
    "\n",
    "#patches = x_4d.unfold(2, patch_size[0], stride[0]).unfold(3, patch_size[1], stride[1])\n",
    "patches = x_4d.unfold(2, patch_size[0], stride[0])\n",
    "print(\"\\nUnfolded Patches:\")\n",
    "print(patches.shape)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "884e8aa1-dff2-4d93-a3b2-fb76e5491de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-18.4258, -18.8908, -19.0066,  -4.8521,   1.7066,   2.3538, -17.8550,\n",
      "          -5.1958,  -0.3300,  18.3806]], grad_fn=<AddmmBackward0>)\n",
      "Image Shape torch.Size([1, 1, 28, 28])\n",
      "Image Shape1 torch.Size([1, 1, 28, 28, 3, 3])\n",
      "Image Shape2 torch.Size([1, 28, 28, 1, 3, 3])\n",
      "torch.Size([1, 1, 30, 30])\n",
      "torch.Size([1, 28, 28, 1, 3, 3])\n",
      "tensor([[-18.4258, -18.8908, -19.0066,  -4.8521,   1.7066,   2.3538, -17.8550,\n",
      "          -5.1958,  -0.3300,  18.3806]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(trainloader):\n",
    "    imgs,_=batch\n",
    "    break\n",
    "img=imgs[0]\n",
    "img=img.unsqueeze(0)\n",
    "op= net(img)\n",
    "print(op)\n",
    "l_idx=0\n",
    "#layer = PatchConvLayer(net.features[l_idx])\n",
    "layer = net.features[l_idx]\n",
    "# Extract all the meta info of the current conv layer.\n",
    "(q, s) = net.features[l_idx].kernel_size\n",
    "(pad1, pad2) = net.features[l_idx].padding  \n",
    "# Truncate all layers before l_idx and wrap the current conv layer as a PatchConvLayer class.\n",
    "(s1, s2) = net.features[l_idx].stride\n",
    "original_shape=img.shape\n",
    "reconstructed_shape = (1,original_shape[1], original_shape[2], original_shape[3])\n",
    "op1= net(unpatchify(patchify1(img,(q, s), (s1,s2),(pad1,pad2)),reconstructed_shape,(q,s),(s1,s2),(pad1,pad2)))\n",
    "print(op1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97a145-2e14-401f-8b58-c0a7601089b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchConvLayer(nn.Module):\n",
    "    def __init__(self, conv_layer):\n",
    "        super().__init__()\n",
    "        self.layer = conv_layer #(k,c,q,s)\n",
    "\n",
    "    def forward(self, patches):\n",
    "        #Todo: 1. Check why the format is nwhcqr when the patches after patchify is nhwcqr.\n",
    "        #      2. Why does this output n,k,w,h when the standard format is n,k,h,w\n",
    "        out = torch.einsum('nwhcqr, kcqr -> nwhk', patches, self.layer.weight)\n",
    "        n, w, h, k = out.shape\n",
    "        out = out.transpose(1, 3).transpose(2, 3) #Should be (n,k,h_out,w_out) even though w and h are swapped\n",
    "        return out\n",
    "\n",
    "for idx, batch in enumerate(trainloader):\n",
    "    imgs,_=batch\n",
    "    break\n",
    "#del img\n",
    "#del(patchnet2)\n",
    "#del(patchnet)\n",
    "#del(patchnet1)\n",
    "\n",
    "img=imgs[4:5]\n",
    "img= img.double()\n",
    "net= net.double()\n",
    "#img=torch.stack([imgs[1],imgs[2]])\n",
    "#print(img)\n",
    "#img=img.unsqueeze(0).to(device)\n",
    "print(net.features[0].weight)\n",
    "torch.cuda.empty_cache()\n",
    "img=img.to(device)\n",
    "print(img.shape)\n",
    "l_idx=3\n",
    "# Extract all the meta info of the current conv layer.\n",
    "(q, s) = net.features[l_idx].kernel_size\n",
    "(pad1, pad2) = net.features[l_idx].padding  \n",
    "(s1, s2) = net.features[l_idx].stride\n",
    "net.to(device)\n",
    "print(net)\n",
    "\n",
    "patchnet = deepcopy(net)\n",
    "temp= deepcopy(net.features[l_idx])\n",
    "layer = PatchConvLayer(temp)\n",
    "patchnet.features = deepcopy(net.features[l_idx:])\n",
    "patchnet.features[0] = layer\n",
    "patchnet.to(device)\n",
    "'''\n",
    "patchnet1 = deepcopy(net)\n",
    "layer = deepcopy(net.features[l_idx])\n",
    "patchnet1.features = deepcopy(net.features[l_idx:])\n",
    "#patchnet1.features[0] = layer\n",
    "patchnet1.features[0].bias = None\n",
    "patchnet1.to(device)\n",
    "'''\n",
    "\n",
    "#print(patchnet2)\n",
    "patchnet2 = deepcopy(net)\n",
    "layer = deepcopy(net.features[l_idx])\n",
    "patchnet2.features = deepcopy(net.features[l_idx:])\n",
    "patchnet2.features[0] = layer\n",
    "patchnet2.features[0].bias = None\n",
    "patchnet2.features[0].padding = (0,0)\n",
    "patchnet2.to(device)\n",
    "print(patchnet2)\n",
    "\n",
    "img = net.features[:l_idx](img)\n",
    "#print(\"shape after run\", img.shape)\n",
    "\n",
    "original_shape = img.shape\n",
    "\n",
    "\n",
    "def single_net(x):\n",
    "            patchnet.eval()\n",
    "            return patchnet(x).squeeze(0)\n",
    "\n",
    "def single_net1(x):\n",
    "            patchnet1.eval()           \n",
    "            reconstructed_shape = (original_shape[0],original_shape[1], original_shape[2], original_shape[3])\n",
    "            x= unpatchify(x,reconstructed_shape,(q,s),(s1,s2),(pad1,pad2))\n",
    "            return patchnet1(x).squeeze(0)\n",
    "\n",
    "def single_net2(x):\n",
    "            patchnet2.eval()\n",
    "            return patchnet2(x).squeeze(0)\n",
    "\n",
    "print(pad1)\n",
    "print(pad2)\n",
    "M = net.features[l_idx].weight\n",
    "_, ki, q, s = M.shape\n",
    "M = M.reshape(-1, ki*q*s)\n",
    "M = torch.einsum('nd, nD -> dD', M, M)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "    temp= patchnet.features[0](patchify1(img,(q, s), (s1,s2),(pad1,pad2)))\n",
    "    op=jacrev(single_net)(patchify1(img,(q, s), (s1,s2),(pad1,pad2)))\n",
    "    n, c, w, h, _, _, _ = op.shape\n",
    "    patches = op.transpose(1, 3).transpose(1, 2) #(n, h_out, w_out, chunk, c, q, s)\n",
    "    grads = patches.reshape(n*w*h, c, -1) #(n*w_out*h_out, chunk, c*q*s)\n",
    "    #print(\"Grads:\",grads.shape)\n",
    "    #Clarify: Where is mean taken\n",
    "    ajop = torch.einsum('ncd, ncD -> dD', grads, grads) #(c*q*s,c*q*s)\n",
    "    print(correlation(ajop, M))\n",
    "    \n",
    "    '''\n",
    "    torch.cuda.empty_cache()\n",
    "    op1=jacrev(single_net1)(patchify1(img,(q, s), (s1,s2),(pad1,pad2)))\n",
    "    n, c, w, h, _, _, _ = op1.shape\n",
    "    patches1 = op1.transpose(1, 3).transpose(1, 2) #(n, h_out, w_out, chunk, c, q, s)\n",
    "    grads1 = patches1.reshape(n*w*h, c, -1) #(n*w_out*h_out, chunk, c*q*s)\n",
    "    #print(\"Grads:\",grads.shape)\n",
    "    #Clarify: Where is mean taken\n",
    "    ajop1 = torch.einsum('ncd, ncD -> dD', grads1, grads1) #(c*q*s,c*q*s)\n",
    "    print(correlation(ajop1,M))\n",
    "    '''\n",
    "    \n",
    "    torch.cuda.empty_cache()  \n",
    "    temp2= patchnet2.features[0](pad(img,(pad2,pad2,pad1,pad1)))\n",
    "    op2=patchify(jacrev(single_net2)(pad(img.squeeze(0),(pad2,pad2,pad1,pad1)).unsqueeze(0)),(q,s),(s1,s2),(pad1,pad2),'dont')\n",
    "    print(op2.shape)\n",
    "    n, c, w, h, _, _, _ = op2.shape\n",
    "    patches2 = op2.transpose(1, 3).transpose(1, 2) #(n, h_out, w_out, chunk, c, q, s)\n",
    "    grads2 = patches2.reshape(n*w*h, c, -1) #(n*w_out*h_out, chunk, c*q*s)\n",
    "    #print(\"Grads:\",grads.shape)\n",
    "    #Clarify: Where is mean taken\n",
    "    ajop2 = torch.einsum('ncd, ncD -> dD', grads2, grads2) #(c*q*s,c*q*s)\n",
    "    print(correlation(ajop2,M))\n",
    "    \n",
    "    print(\"patch\",temp)\n",
    "    print(\"patch2\",temp2)\n",
    "    \n",
    "    \n",
    "    #With rtol 0.5 this was giving True. So there is 0.4 difference in these tensors. \n",
    "    #Double precision solved the issue\n",
    "    print(torch.allclose(temp2, temp, rtol=0.00000001, atol=0, equal_nan=False))\n",
    "    #print(torch.allclose(temp2, temp))\n",
    "\n",
    "#t1= pad(img,(pad2,pad2,pad1,pad1))\n",
    "#t2= \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af58fb-1830-440e-aa89-1f28d3a3450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = torch.tensor([[1.98998, 15., 3.],\n",
    "                     # [4.89898, 5., 6.,],\n",
    "                     # [10.54554, 8., 9.]])\n",
    "#wt= torch.tensor([[[10.55665,300.435345345],\n",
    "                  #[6.345345345345345345,4.34543]],\n",
    "                   #[[[13.,5.],\n",
    "                  #[6.,3.]]]])\n",
    "\n",
    "class PatchConvLayer(nn.Module):\n",
    "    def __init__(self, conv_layer):\n",
    "        super().__init__()\n",
    "        self.layer = conv_layer #(k,c,q,s)\n",
    "\n",
    "    def forward(self, patches):\n",
    "        #Todo: 1. Check why the format is nwhcqr when the patches after patchify is nhwcqr.\n",
    "        #      2. Why does this output n,k,w,h when the standard format is n,k,h,w\n",
    "        out = torch.einsum('nwhcqr, kcqr -> nwhk', patches, self.layer.weight)\n",
    "        n, w, h, k = out.shape\n",
    "        out = out.transpose(1, 3).transpose(2, 3) #Should be (n,k,h_out,w_out) even though w and h are swapped\n",
    "        return out\n",
    "\n",
    "inc=5\n",
    "outc= 500\n",
    "ker=7\n",
    "ip=100\n",
    "data = torch.randn(1, inc, ip, ip, dtype=torch.float64)\n",
    "wt = torch.randn(outc, inc, ker, ker, dtype=torch.float64)\n",
    "datap=pad(data.squeeze(0),(1,1,1,1)).unsqueeze(0)\n",
    "#data= data.unsqueeze(0).unsqueeze(0) \n",
    "#datap= datap.unsqueeze(0).unsqueeze(0) \n",
    "patches= patchify1(data,(ker,ker),(1,1),(1,1))\n",
    "net1= nn.Conv2d(in_channels=inc, out_channels=outc, kernel_size=(ker,ker), stride=(1,1), padding=(1,1))\n",
    "layer=deepcopy(net1)\n",
    "layer.weight= nn.Parameter(wt)\n",
    "layer.bias= None\n",
    "t1= PatchConvLayer(layer)\n",
    "#print(data)\n",
    "res1= t1.forward(patches)\n",
    "print(res1)\n",
    "\n",
    "layer1 = deepcopy(net1)\n",
    "layer1.weight= nn.Parameter(wt)\n",
    "layer1.bias= None\n",
    "layer1.padding=(0,0)\n",
    "res2=layer1(datap)\n",
    "\n",
    "\n",
    "print(res2)\n",
    "print(torch.allclose(res2, res1, rtol=0.000001, atol=0, equal_nan=False))\n",
    "print(torch.allclose(res2, res1))\n",
    "torch.eq(res1,res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "cdb6ccf5-195a-484a-a67c-1967a054bbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor([[5., 9.],\n",
      "        [9., 5.]])\n",
      "tensor([[5., 6.],\n",
      "        [3., 5.]])\n"
     ]
    }
   ],
   "source": [
    "ip=torch.tensor([2.0,3.0,5.0])\n",
    "ip1= torch.tensor([[ip[0],ip[1]],[ip[1], ip[2]]])\n",
    "print(ip1[0][0])\n",
    "#ip1= nn.Parameter(ip1)\n",
    "def fn1(ip):\n",
    "    return ip[0]*5.0+ip[1]*6.0 + ip[1]*3.0+ ip[2]*5.0\n",
    "def fn2(ip):\n",
    "    return ip[0][0]*5.0 +ip[0][1]*6.0 + ip[1][0]*3.0+ ip[1][1]*5.0\n",
    "opt1 = jacrev(fn1)(ip)\n",
    "opt1 = torch.tensor([[opt1[0],opt1[1]],[opt1[1], opt1[2]]])\n",
    "opt2 = jacrev(fn2)(ip1)\n",
    "\n",
    "print(opt1)\n",
    "print(opt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19cf88e-c635-4f5e-8cef-bdc0a4277a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
