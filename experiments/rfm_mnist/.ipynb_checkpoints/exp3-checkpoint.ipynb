{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ae23a4c-e713-4039-9d80-b967a7353fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "parent_dir='C:\\\\Users\\\\garav\\\\AGOP\\\\DLR'\n",
    "model_dir= 'C:\\\\Users\\\\garav\\\\AGOP\\\\DLR\\\\trained_models\\\\MNIST\\\\model4\\\\nn_models\\\\'\n",
    "#parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0dbcae2-852a-4c16-a540-9c9c2dd292cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Without the incoming socket you cannot receive events from the server or register event handlers to your Visdom client.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garav\\AGOP\\DLR\\experiments\\rfm_mnist\n",
      "C:\\Users\\garav\\AGOP\\DLR\n",
      "C:\\Users\\garav\\AGOP\\DLR\\trained_models\\MNIST\\model3\\nn_models\\\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from utils import agop_gcnn as agc\n",
    "from utils import trainer as tr\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import rfm\n",
    "import numpy as np\n",
    "from trained_models.MNIST.model4 import trainer as t\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.linalg import norm\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from utils.groupy.gconv.pytorch_gconv.splitgconv2d import P4ConvZ2, P4ConvP4, P4MConvZ2, P4MConvP4M\n",
    "from groupy.gconv.make_gconv_indices import *\n",
    "from copy import deepcopy\n",
    "from torch.nn.functional import pad\n",
    "from torch.func import jacrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd80ae41-ffa3-4ae8-87ab-8452edb259d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c92e10d-06bf-41fb-8580-c35bf7601fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83afe4f3-6178-42d2-a0fe-cef3228c6760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "trainloader, valloader, testloader = t.get_loaders()\n",
    "net= t.get_untrained_net()\n",
    "init_net= deepcopy(net)\n",
    "import os\n",
    "if os.path.exists(model_dir+'mnist_gcnn_trained_nn.pth'):\n",
    "    checkpoint = torch.load(model_dir+'mnist_gcnn_trained_nn.pth', map_location=torch.device(device))\n",
    "    net.load_state_dict(checkpoint['state_dict'])  # Access the 'state_dict' within the loaded dictionary\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "\n",
    "#print(\"Train the network first\")\n",
    "#t.train_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a1ba576-4192-4bd2-8d6e-dc55bfcf50d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garav\\AGOP\\DLR\\trained_models\\MNIST\\model4\\model4.py:44: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "tr.visualize_predictions(net, testloader, range(10), device, num_images=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e55ea34-a974-439d-8fa6-d68979ce388b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_idx 0\n",
      "Computing GOP for sample 0 out of 10\n",
      "Computing GOP for sample 1 out of 10\n",
      "Computing GOP for sample 2 out of 10\n",
      "Computing GOP for sample 3 out of 10\n",
      "Computing GOP for sample 4 out of 10\n",
      "Computing GOP for sample 5 out of 10\n",
      "Computing GOP for sample 6 out of 10\n",
      "Computing GOP for sample 7 out of 10\n",
      "Computing GOP for sample 8 out of 10\n",
      "Computing GOP for sample 9 out of 10\n",
      "Computing GOP for sample 10 out of 10\n",
      "Shape after gradients:  torch.Size([9, 9])\n",
      "Correlation between Trained Gcnn and AGOP:  tensor(0.9710, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Correlation between Un-trained Gcnn and AGOP:  tensor(0.7090, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 32.8832,  11.0137,  -0.2061,  14.9455,   9.8338,  -8.2748,  -0.8288,\n",
       "          -7.7451, -13.8880],\n",
       "        [ 11.0137,  40.9705,  15.7780,  -2.2490,  10.3861,  -1.0840, -10.1144,\n",
       "         -11.8915,  -7.0418],\n",
       "        [ -0.2061,  15.7780,  34.8157,  -8.3311,  10.1940,  12.0150, -15.8977,\n",
       "          -9.9664,  -0.3288],\n",
       "        [ 14.9455,  -2.2490,  -8.3311,  40.5436,  10.5808, -11.4767,  12.1232,\n",
       "          -1.3768,  -9.2086],\n",
       "        [  9.8338,  10.3861,  10.1940,  10.5808,  61.7055,  10.3711,  10.2591,\n",
       "          10.8842,  10.0160],\n",
       "        [ -8.2748,  -1.0840,  12.0150, -11.4767,  10.3711,  40.4735,  -8.5745,\n",
       "          -2.9043,  14.5416],\n",
       "        [ -0.8288, -10.1144, -15.8977,  12.1232,  10.2591,  -8.5745,  35.3570,\n",
       "          16.6435,  -0.5595],\n",
       "        [ -7.7451, -11.8915,  -9.9664,  -1.3768,  10.8842,  -2.9043,  16.6435,\n",
       "          42.0940,  11.3312],\n",
       "        [-13.8880,  -7.0418,  -0.3288,  -9.2086,  10.0160,  14.5416,  -0.5595,\n",
       "          11.3312,  32.8482]], dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agc.verify_NFA(net, init_net, trainloader, layer_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bf080c8-f1a8-4fe7-8bec-83c2c8c17a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_idx 2\n",
      "Computing GOP for sample 0 out of 10\n",
      "Computing GOP for sample 1 out of 10\n",
      "Computing GOP for sample 2 out of 10\n",
      "Computing GOP for sample 3 out of 10\n",
      "Computing GOP for sample 4 out of 10\n",
      "Computing GOP for sample 5 out of 10\n",
      "Computing GOP for sample 6 out of 10\n",
      "Computing GOP for sample 7 out of 10\n",
      "Computing GOP for sample 8 out of 10\n",
      "Computing GOP for sample 9 out of 10\n",
      "Computing GOP for sample 10 out of 10\n",
      "Shape after gradients:  torch.Size([360, 360])\n",
      "Correlation between Trained Gcnn and AGOP:  tensor(0.9770, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Correlation between Un-trained Gcnn and AGOP:  tensor(0.0836, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5122,  0.5156,  0.1316,  ..., -0.3701,  0.1405, -0.4765],\n",
       "        [ 0.5156,  1.7492,  0.1980,  ...,  0.2609,  0.0725, -0.2701],\n",
       "        [ 0.1316,  0.1980,  1.8413,  ..., -0.6061, -0.3391, -0.5612],\n",
       "        ...,\n",
       "        [-0.3701,  0.2609, -0.6061,  ...,  3.3044,  0.7460, -0.0309],\n",
       "        [ 0.1405,  0.0725, -0.3391,  ...,  0.7460,  2.1470,  0.8858],\n",
       "        [-0.4765, -0.2701, -0.5612,  ..., -0.0309,  0.8858,  2.7916]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agc.verify_NFA(net, init_net, trainloader, layer_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d13cd617-9c6c-4b83-bdab-28098d088e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_idx 5\n",
      "Computing GOP for sample 0 out of 10\n",
      "Computing GOP for sample 1 out of 10\n",
      "Computing GOP for sample 2 out of 10\n",
      "Computing GOP for sample 3 out of 10\n",
      "Computing GOP for sample 4 out of 10\n",
      "Computing GOP for sample 5 out of 10\n",
      "Computing GOP for sample 6 out of 10\n",
      "Computing GOP for sample 7 out of 10\n",
      "Computing GOP for sample 8 out of 10\n",
      "Computing GOP for sample 9 out of 10\n",
      "Computing GOP for sample 10 out of 10\n",
      "Shape after gradients:  torch.Size([360, 360])\n",
      "Correlation between Trained Gcnn and AGOP:  tensor(0.9334, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Correlation between Un-trained Gcnn and AGOP:  tensor(0.1745, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4325,  0.1340, -0.2771,  ..., -0.0348, -0.1890,  0.1651],\n",
       "        [ 0.1340,  1.2878, -0.2561,  ...,  0.2383, -0.0286,  0.0565],\n",
       "        [-0.2771, -0.2561,  1.2542,  ..., -0.1439,  0.0051, -0.0365],\n",
       "        ...,\n",
       "        [-0.0348,  0.2383, -0.1439,  ...,  1.5052,  0.0078, -0.0170],\n",
       "        [-0.1890, -0.0286,  0.0051,  ...,  0.0078,  1.3672,  0.2067],\n",
       "        [ 0.1651,  0.0565, -0.0365,  ..., -0.0170,  0.2067,  1.4088]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agc.verify_NFA(net, init_net, trainloader, layer_idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1667356-c64f-415a-ad0c-1d4e9df5d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_idx 7\n",
      "Computing GOP for sample 0 out of 10\n",
      "Computing GOP for sample 1 out of 10\n",
      "Computing GOP for sample 2 out of 10\n",
      "Computing GOP for sample 3 out of 10\n",
      "Computing GOP for sample 4 out of 10\n",
      "Computing GOP for sample 5 out of 10\n",
      "Computing GOP for sample 6 out of 10\n",
      "Computing GOP for sample 7 out of 10\n",
      "Computing GOP for sample 8 out of 10\n",
      "Computing GOP for sample 9 out of 10\n",
      "Computing GOP for sample 10 out of 10\n",
      "Shape after gradients:  torch.Size([720, 720])\n",
      "Correlation between Trained Gcnn and AGOP:  tensor(0.9653, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Correlation between Un-trained Gcnn and AGOP:  tensor(0.0986, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8657,  0.0083,  0.2049,  ..., -0.1020, -0.0330,  0.0882],\n",
       "        [ 0.0083,  0.8676,  0.0457,  ..., -0.1038,  0.3065,  0.1029],\n",
       "        [ 0.2049,  0.0457,  0.8468,  ...,  0.0284,  0.0456,  0.1190],\n",
       "        ...,\n",
       "        [-0.1020, -0.1038,  0.0284,  ...,  0.9005,  0.0070, -0.0297],\n",
       "        [-0.0330,  0.3065,  0.0456,  ...,  0.0070,  0.7943, -0.0055],\n",
       "        [ 0.0882,  0.1029,  0.1190,  ..., -0.0297, -0.0055,  0.7218]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agc.verify_NFA(net, init_net, trainloader, layer_idx=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f378a-11e2-425b-844c-2c27dc59c569",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Debug Task: Debug gcnn for AGOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1016c9c-9f91-4650-8e54-c3013b26ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_filter(w, inds):\n",
    "    inds_reshape = inds.reshape((-1, inds.shape[-1])).astype(np.int64)\n",
    "    w_indexed = w[:, :, inds_reshape[:, 0].tolist(), inds_reshape[:, 1].tolist(), inds_reshape[:, 2].tolist()]\n",
    "    w_indexed = w_indexed.view(w_indexed.size()[0], w_indexed.size()[1],\n",
    "                                    inds.shape[0], inds.shape[1], inds.shape[2], inds.shape[3])\n",
    "    w_transformed = w_indexed.permute(0, 2, 1, 3, 4, 5)\n",
    "    return w_transformed.contiguous()\n",
    "'''\n",
    "layer_idx=2\n",
    "count=0\n",
    "# Get the layer_idx+1 th conv layer\n",
    "for idx, m in enumerate(net.children()):\n",
    "        if isinstance(m, P4ConvZ2):\n",
    "            print(\"hi\")\n",
    "            count += 1\n",
    "        if count-1 == layer_idx:\n",
    "            l_idx = idx\n",
    "      break\n",
    "            '''\n",
    "l_idx =7\n",
    "layer = deepcopy(net.features[l_idx])\n",
    "print(l_idx)\n",
    "print(layer.weight)\n",
    "print(layer.kernel_size)\n",
    "print(layer.padding)\n",
    "print(layer.stride)\n",
    "# Extract W matrix\n",
    "tw = trans_filter(layer.weight, layer.inds)\n",
    "tw_shape = (layer.out_channels * layer.output_stabilizer_size,\n",
    "                    layer.in_channels * layer.input_stabilizer_size,\n",
    "                    layer.ksize, layer.ksize)\n",
    "M = tw.view(tw_shape)\n",
    "\n",
    "\n",
    "#_, ki, q, s = M.shape\n",
    "#print(M.shape)\n",
    "#_,ki, q, s = M.shape\n",
    "\n",
    "k, ki, q,s= M.shape\n",
    "            \n",
    "# Build W which is a (k, c*q*s) matrix. What to do with ip_stab\n",
    "M = M.reshape(-1, ki*q*s)\n",
    "            \n",
    "# Compute WtW which is (c*q*s,c*q*s) matrix\n",
    "M = torch.einsum('nd, nD -> dD', M, M)\n",
    "\n",
    "\n",
    "print(M)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38026029-08a2-468c-af15-05f96f61810e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def correlation(M1, M2):\n",
    "    M1 -= M1.mean()\n",
    "    M2 -= M2.mean()\n",
    "\n",
    "    norm1 = norm(M1.flatten())\n",
    "    norm2 = norm(M2.flatten())\n",
    "\n",
    "    return torch.sum(M1.cuda() * M2.cuda()) / (norm1 * norm2)\n",
    "\n",
    "def patchify(x, in_channels, ip_stab, patch_size, stride_size, padding=None, pad_type='zeros'):\n",
    "    '''\n",
    "        Given an input image (n,c,h,w) generate (n,w_out,h_out,c,q,s) respecting stride,padding, \n",
    "        w_out is number of pathces along the width for the given stride after padding\n",
    "        h_out is number of pathces along the height for the given stride after padding\n",
    "        (q,s) is the kernel dimensions \n",
    "    '''\n",
    "    input_shape = x.size()\n",
    "    #TODO: The last two shapes look swapped. This was the same order in cohens code too. For square ips \n",
    "    #there is no effect. However for rect ips what would happen?\n",
    "    x = x.view(input_shape[0], in_channels*ip_stab, input_shape[-2], input_shape[-1])\n",
    "    #x = x.view(input_shape[0], in_channels*ip_stab, input_shape[-2], input_shape[-1])\n",
    "    q1, q2 = patch_size\n",
    "    s1, s2 = stride_size\n",
    "    print(\"Image Shape\",x.shape)\n",
    "    if padding is None:\n",
    "        pad_1 = (q1-1)//2\n",
    "        pad_2 = (q2-1)//2\n",
    "    else:\n",
    "        pad_1, pad_2 = padding\n",
    "\n",
    "    pad_dims = (pad_2, pad_2, pad_1, pad_1)\n",
    "    if pad_type == 'zeros':\n",
    "        x = pad(x, pad_dims)\n",
    "    elif pad_type == 'circular':\n",
    "        x = pad(x, pad_dims, 'circular')\n",
    "        \n",
    "    patches = x.unfold(2, q1, s1).unfold(3, q2, s2) #(n, c, h_out, w_out, q, s)\n",
    "    print(\"Image Shape1\",patches.shape)\n",
    "    patches = patches.transpose(1, 3).transpose(1, 2) #(n,w_out,h_out,c,q,s) \n",
    "    print(\"Image Shape2\",patches.shape)\n",
    "    return patches\n",
    "\n",
    "def trans_filter(w, inds):\n",
    "    inds_reshape = inds.reshape((-1, inds.shape[-1])).astype(np.int64)\n",
    "    w_indexed = w[:, :, inds_reshape[:, 0].tolist(), inds_reshape[:, 1].tolist(), inds_reshape[:, 2].tolist()]\n",
    "    w_indexed = w_indexed.view(w_indexed.size()[0], w_indexed.size()[1],\n",
    "                                    inds.shape[0], inds.shape[1], inds.shape[2], inds.shape[3])\n",
    "    w_transformed = w_indexed.permute(0, 2, 1, 3, 4, 5)\n",
    "    return w_transformed.contiguous()\n",
    "\n",
    "class PatchConvLayer(nn.Module):\n",
    "    def __init__(self, conv_layer):\n",
    "        super().__init__()\n",
    "        self.layer = conv_layer #(k,c,q,s)\n",
    "        #inds = make_c4_z2_indices(self.layer.ksize)\n",
    "       \n",
    "    def forward(self, patches):\n",
    "        tw = trans_filter(self.layer.weight, self.layer.inds)\n",
    "        tw_shape = (self.layer.out_channels * self.layer.output_stabilizer_size,\n",
    "                    self.layer.in_channels * self.layer.input_stabilizer_size,\n",
    "                    self.layer.ksize, self.layer.ksize)\n",
    "        tw = tw.view(tw_shape)\n",
    "        print(\"tw shape\",tw.shape)\n",
    "        print(\"Patch_shape\", patches.shape)\n",
    "        #n, _, ny_out, nx_out = patches.shape()\n",
    "        #print(tw)\n",
    "        out = torch.einsum('nhwcqr, kcqr -> nhwk', patches, tw)\n",
    "        n, w, h, k = out.shape\n",
    "        out = out.transpose(1, 3).transpose(2, 3) #(n,k,h_out,w_out)\n",
    "        out = out.view(n, self.layer.out_channels, self.layer.output_stabilizer_size, h, w)\n",
    "        print(\"out_shape\", out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "for idx, batch in enumerate(trainloader):\n",
    "    imgs,_=batch\n",
    "    break\n",
    "\n",
    "img=imgs[6:7]\n",
    "l_idx=7\n",
    "img=img.to(device)\n",
    "img = img.double()\n",
    "#layers= list(net.children())\n",
    "#print(\"Shape of wight matrix:\",layers[l_idx].weight.shape)\n",
    "net=net.double()\n",
    "net = net.to(device)\n",
    "img = net.features[:l_idx](img)\n",
    "print(\"Image shape\",img.shape)\n",
    "\n",
    "\n",
    "\n",
    "patchnet = deepcopy(net)\n",
    "temp= deepcopy(net.features[l_idx])\n",
    "layer = PatchConvLayer(temp)\n",
    "patchnet.features = deepcopy(net.features[l_idx:])\n",
    "patchnet.features[0] = layer\n",
    "patchnet.to(device)\n",
    "#op = patchnet(patchify(img, temp.kernel_size, temp.stride, temp.padding))\n",
    "op1 = patchnet.features[0](patchify(img, temp.in_channels, \n",
    "                                    temp.input_stabilizer_size, temp.kernel_size, temp.stride, temp.padding))\n",
    "print(op1.shape)\n",
    "print(op1)\n",
    "#print(patchnet2)\n",
    "\n",
    "\n",
    "patchnet2 = deepcopy(net)\n",
    "layer = deepcopy(net.features[l_idx])\n",
    "patchnet2.features = deepcopy(net.features[l_idx:])\n",
    "patchnet2.features[0] = layer\n",
    "patchnet2.features[0].bias = None\n",
    "patchnet2.features[0].padding = (0,0)\n",
    "patchnet2.to(device)\n",
    "#print(patchnet2)\n",
    "#op2= patchnet2(img)\n",
    "op2= patchnet2.features[0](img)\n",
    "#print(op2.shape)\n",
    "#print(op2)\n",
    "print(torch.allclose(op1,op2,rtol=0.00000000001,atol=0, equal_nan=False))\n",
    "\n",
    "def single_net(x):\n",
    "            patchnet.eval()\n",
    "            return patchnet(x).squeeze(0)\n",
    "\n",
    "def single_net2(x):\n",
    "            patchnet2.eval()\n",
    "            return patchnet2(x).squeeze(0)\n",
    "\n",
    "op=jacrev(single_net)(patchify(img, temp.in_channels, \n",
    "                                    temp.input_stabilizer_size, temp.kernel_size, temp.stride, temp.padding))\n",
    "n, c, w, h, _, _, _ = op.shape\n",
    "patches = op.transpose(1, 3).transpose(1, 2) #(n, h_out, w_out, chunk, c, q, s)\n",
    "grads = patches.reshape(n*w*h, c, -1) #(n*w_out*h_out, chunk, c*q*s)\n",
    "#print(\"Grads:\",grads.shape)\n",
    "#Clarify: Where is mean taken\n",
    "ajop = torch.einsum('ncd, ncD -> dD', grads, grads) #(c*q*s,c*q*s)\n",
    "ajop= sqrt(ajop)\n",
    "print(correlation(ajop, M))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d1b36924-67a6-4f9e-853e-0018f3bb1e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1223, -1.2220,  1.0906],\n",
      "        [ 0.2609, -1.7309, -1.0689],\n",
      "        [-1.8210,  1.9365, -1.7288]])\n",
      "tensor([[-0.1223, -1.2220,  1.0906],\n",
      "        [ 0.2609, -1.7309, -1.0689],\n",
      "        [-1.8210,  1.9365, -1.7288]])\n"
     ]
    }
   ],
   "source": [
    "def co(M1):\n",
    "    M= M1.clone()\n",
    "    M -= M.mean()\n",
    "    #M.flatten()\n",
    "    return M\n",
    "M= torch.randn(3, 3)\n",
    "print(M)\n",
    "co(M)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01770270-f3e7-4246-b08f-9620340b9aa8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Debug Task: Check if correlation existst between WTW and AGOP with W not including rotationsÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34d99c10-c781-4de8-a7ef-c1729d0d9382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Without the incoming socket you cannot receive events from the server or register event handlers to your Visdom client.\n"
     ]
    }
   ],
   "source": [
    "''' This module does the following\n",
    "1. Scan the network for conv layers\n",
    "2. For each gcnn conv layer compute W^TW of eq 3\n",
    "3. For each gcnn conv layer compute the AGOP(AJOP in case of multiple outputs)\n",
    "4. For each gcnn conv layer print the pearson correlation between 2 and 3\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "#from functorch import jacrev, vmap\n",
    "from torch.func import jacrev\n",
    "from torch.nn.functional import pad\n",
    "#import dataset\n",
    "#from numpy.linalg import eig\n",
    "from copy import deepcopy\n",
    "from torch.linalg import norm, svd\n",
    "from torchvision import models\n",
    "import visdom\n",
    "from torch.linalg import norm, eig\n",
    "#import torchvision\n",
    "#import torchvision.transforms as transforms\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.linalg import norm\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from utils.groupy.gconv.pytorch_gconv.splitgconv2d import P4ConvZ2, P4ConvP4, P4MConvZ2, P4MConvP4M\n",
    "from groupy.gconv.make_gconv_indices import *\n",
    "from copy import deepcopy\n",
    "from torch.nn.functional import pad\n",
    "from torch.func import jacrev\n",
    "\n",
    "SEED = 2323\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "vis = visdom.Visdom('http://127.0.0.1', use_incoming_socket=False)\n",
    "vis.close(env='main')\n",
    "\n",
    "def patchify(x, in_channels, ip_stab, patch_size, stride_size, padding=None, pad_type='zeros'):\n",
    "    '''\n",
    "        Given an input image (n,c,h,w) generate (n,w_out,h_out,c,q,s) respecting stride,padding, \n",
    "        w_out is number of pathces along the width for the given stride after padding\n",
    "        h_out is number of pathces along the height for the given stride after padding\n",
    "        (q,s) is the kernel dimensions \n",
    "    '''\n",
    "    input_shape = x.size()\n",
    "    #TODO: The last two shapes look swapped. This was the same order in cohens code too. For square ips \n",
    "    #there is no effect. However for rect ips what would happen?\n",
    "    x = x.view(input_shape[0], in_channels*ip_stab, input_shape[-2], input_shape[-1])\n",
    "    #x = x.view(input_shape[0], in_channels*ip_stab, input_shape[-2], input_shape[-1])\n",
    "    q1, q2 = patch_size\n",
    "    s1, s2 = stride_size\n",
    "    #print(\"Image Shape\",x.shape)\n",
    "    if padding is None:\n",
    "        pad_1 = (q1-1)//2\n",
    "        pad_2 = (q2-1)//2\n",
    "    else:\n",
    "        pad_1, pad_2 = padding\n",
    "\n",
    "    pad_dims = (pad_2, pad_2, pad_1, pad_1)\n",
    "    if pad_type == 'zeros':\n",
    "        x = pad(x, pad_dims)\n",
    "    elif pad_type == 'circular':\n",
    "        x = pad(x, pad_dims, 'circular')\n",
    "        \n",
    "    patches = x.unfold(2, q1, s1).unfold(3, q2, s2) #(n, c, h_out, w_out, q, s)\n",
    "    #print(\"Image Shape1\",patches.shape)\n",
    "    patches = patches.transpose(1, 3).transpose(1, 2) #(n,w_out,h_out,c,q,s) \n",
    "    #print(\"Image Shape2\",patches.shape)\n",
    "    return patches\n",
    "\n",
    "def trans_filter(w, inds):\n",
    "    inds_reshape = inds.reshape((-1, inds.shape[-1])).astype(np.int64)\n",
    "    w_indexed = w[:, :, inds_reshape[:, 0].tolist(), inds_reshape[:, 1].tolist(), inds_reshape[:, 2].tolist()]\n",
    "    w_indexed = w_indexed.view(w_indexed.size()[0], w_indexed.size()[1],\n",
    "                                    inds.shape[0], inds.shape[1], inds.shape[2], inds.shape[3])\n",
    "    w_transformed = w_indexed.permute(0, 2, 1, 3, 4, 5)\n",
    "    return w_transformed.contiguous()\n",
    "    \n",
    "class PatchConvLayer(nn.Module):\n",
    "    def __init__(self, conv_layer):\n",
    "        super().__init__()\n",
    "        self.layer = conv_layer #(k,c,q,s)\n",
    "        #inds = make_c4_z2_indices(self.layer.ksize)\n",
    "       \n",
    "    def forward(self, patches):\n",
    "        tw = trans_filter(self.layer.weight, self.layer.inds)\n",
    "        tw_shape = (self.layer.out_channels * self.layer.output_stabilizer_size,\n",
    "                    self.layer.in_channels * self.layer.input_stabilizer_size,\n",
    "                    self.layer.ksize, self.layer.ksize)\n",
    "        tw = tw.view(tw_shape)\n",
    "        #print(\"tw shape\",tw.shape)\n",
    "        #print(\"Patch_shape\", patches.shape)\n",
    "        out = torch.einsum('nhwcqr, kcqr -> nhwk', patches, tw)\n",
    "        n, w, h, k = out.shape\n",
    "        out = out.transpose(1, 3).transpose(2, 3) #(n,k,h_out,w_out)\n",
    "        out = out.view(n, self.layer.out_channels, self.layer.output_stabilizer_size, h, w)\n",
    "        #print(\"out_shape\", out.shape)\n",
    "        return out\n",
    "\n",
    "def get_jacobian(net, data, c_idx=0, chunk=100):\n",
    "    with torch.no_grad():\n",
    "        def single_net(x):\n",
    "            # x is (w_out,h_out,c,q,s)\n",
    "            return net(x.unsqueeze(0))[:,c_idx*chunk:(c_idx+1)*chunk].squeeze(0)\n",
    "        # Parallelize across the images.\n",
    "        return torch.vmap(jacrev(single_net))(data) #(n, chunk, w_out, h_out, c, q, s)\n",
    "\n",
    "def egop(model, z):\n",
    "    ajop = 0\n",
    "    c = 10\n",
    "    chunk_idxs = 1\n",
    "    #Chunking is done to compute jacobian as chunks. This saves memory\n",
    "    #TODO: chunk should be passed as argument\n",
    "    chunk = c // chunk_idxs\n",
    "    for i in range(chunk_idxs):\n",
    "        J = get_jacobian(model, z, c_idx=i, chunk=chunk)\n",
    "        n, c, w, h, _, _, _ = J.shape\n",
    "        J = J.transpose(1, 3).transpose(1, 2) #(n, w_out, h_out, chunk, c, q, s)\n",
    "        grads = J.reshape(n*w*h, c, -1) #(n*w_out*h_out, chunk, c*q*s)\n",
    "        #Clarify: Where is mean taken\n",
    "        ajop += torch.einsum('ncd, ncD -> dD', grads, grads) #(c*q*s,c*q*s)\n",
    "    return ajop\n",
    "\n",
    "\n",
    "def load_nn(net, init_net, layer_idx=0):\n",
    "    \n",
    "    count = 0\n",
    "    # Get the layer_idx+1 th conv layer\n",
    "    for idx, m in enumerate(net.features):\n",
    "        if isinstance(m, (P4ConvZ2, P4ConvP4, P4MConvZ2, P4MConvP4M)):\n",
    "            count += 1\n",
    "        if count-1 == layer_idx:\n",
    "            l_idx = idx\n",
    "            break\n",
    "\n",
    "    print(\"l_idx\",l_idx)\n",
    "    layer = deepcopy(net.features[l_idx])\n",
    "    layer_init = deepcopy(init_net.features[l_idx])\n",
    "\n",
    "    # Extract all the meta info of the current conv layer.\n",
    "    (q, s) = net.features[l_idx].kernel_size\n",
    "    (pad1, pad2) = net.features[l_idx].padding\n",
    "    (s1, s2) = net.features[l_idx].stride \n",
    "    in_channels = layer.in_channels\n",
    "    input_stabilizer_size = layer.input_stabilizer_size\n",
    "    \n",
    "    # Extract W matrix\n",
    "    #tw = trans_filter(layer.weight, layer.inds)\n",
    "    tw = layer.weight\n",
    "    tw_shape = (layer.out_channels,\n",
    "                        layer.in_channels * layer.input_stabilizer_size,\n",
    "                        layer.ksize, layer.ksize)\n",
    "    M = tw.view(tw_shape)\n",
    "    \n",
    "    #tw= trans_filter(layer_init.weight, layer_init.inds)\n",
    "    tw = layer_init.weight\n",
    "    tw_shape = (layer_init.out_channels,\n",
    "                        layer_init.in_channels * layer_init.input_stabilizer_size,\n",
    "                        layer_init.ksize, layer_init.ksize)\n",
    "    M0 = tw.view(tw_shape)\n",
    "    \n",
    "    k, ki, q,s= M.shape\n",
    "                \n",
    "    # Build W which is a (k, c*q*s) matrix. What to do with ip_stab\n",
    "    M = M.reshape(-1, ki*q*s)\n",
    "                \n",
    "    # Compute WtW which is (c*q*s,c*q*s) matrix\n",
    "    M = torch.einsum('nd, nD -> dD', M, M)\n",
    "\n",
    "    k, ki, q,s= M0.shape\n",
    "\n",
    "    # Build W which is a (k, c*q*s) matrix. What to do with ip_stab\n",
    "    M0 = M0.reshape(-1, ki*q*s)\n",
    "\n",
    "    # Compute WtW which is (c*q*s,c*q*s) matrix\n",
    "    M0 = torch.einsum('nd, nD -> dD', M0, M0)\n",
    "\n",
    "    # Construct patchnet\n",
    "    patchnet = deepcopy(net)\n",
    "    temp = deepcopy(net.features[l_idx])\n",
    "    layer = PatchConvLayer(temp)\n",
    "    \n",
    "    # Truncate all layers before l_idx    \n",
    "    patchnet.features = net.features[l_idx:]\n",
    "    patchnet.features[0] = layer\n",
    "\n",
    "    return net, patchnet, M, M0, l_idx, [(q, s), (pad1,pad2), (s1,s2)], in_channels, input_stabilizer_size\n",
    "\n",
    "\n",
    "def get_grads(net, in_channels, input_stabilizer_size, patchnet, trainloader,\n",
    "              kernel=(3,3), padding=(1,1),\n",
    "              stride=(1,1), layer_idx=0):\n",
    "    net.eval()\n",
    "    net.cuda()\n",
    "    patchnet.eval()\n",
    "    patchnet.cuda()\n",
    "    M = 0\n",
    "    q, s = kernel\n",
    "    pad1, pad2 = padding\n",
    "    s1, s2 = stride\n",
    "\n",
    "    # Num images for taking AGOP (Can be small for early layers)\n",
    "    MAX_NUM_IMGS = 10\n",
    "\n",
    "    for idx, batch in enumerate(trainloader):\n",
    "        print(\"Computing GOP for sample \" + str(idx) + \\\n",
    "              \" out of \" + str(MAX_NUM_IMGS))\n",
    "        imgs, _ = batch\n",
    "        imgs= imgs.double()\n",
    "        with torch.no_grad():\n",
    "            imgs = imgs.cuda()        \n",
    "            # Run the first half of the network wrt to the current layer \n",
    "            imgs = net.features[:layer_idx](imgs).cpu() #(n,c,h,w)\n",
    "        patches = patchify(imgs, in_channels, input_stabilizer_size, \n",
    "                           (q, s), (s1,s2), padding=(pad1,pad2))#(n,w_out,h_out,c,q,s)\n",
    "        patches = patches.cuda()\n",
    "        #print(patches.shape)\n",
    "        M += egop(patchnet, patches).cpu()\n",
    "        del imgs, patches\n",
    "        torch.cuda.empty_cache()\n",
    "        if idx >= MAX_NUM_IMGS:\n",
    "            break\n",
    "    net.cpu()\n",
    "    patchnet.cpu()\n",
    "    return M\n",
    "\n",
    "\n",
    "def min_max(M):\n",
    "    return (M - M.min()) / (M.max() - M.min())\n",
    "\n",
    "\n",
    "def correlation(A, B):\n",
    "    M1 = A.clone()\n",
    "    M2 = B.clone()\n",
    "    M1 -= M1.mean()\n",
    "    M2 -= M2.mean()\n",
    "\n",
    "    norm1 = norm(M1.flatten())\n",
    "    norm2 = norm(M2.flatten())\n",
    "\n",
    "    return torch.sum(M1.cuda() * M2.cuda()) / (norm1 * norm2)\n",
    "\n",
    "\n",
    "def verify_NFA(net, init_net, trainloader, layer_idx=0):\n",
    "\n",
    "    net = net.double()\n",
    "    init_net = init_net.double()\n",
    "    net, patchnet, M, M0, l_idx, conv_vals, in_channels, input_stabilizer_size = load_nn(net,\n",
    "                                                     init_net,\n",
    "                                                     layer_idx=layer_idx)\n",
    "    (q, s), (pad1, pad2), (s1, s2) = conv_vals\n",
    "    '''\n",
    "    i_val = correlation(M0, M)\n",
    "    print(\"Correlation between Initial and Trained CNFM: \", i_val)'''\n",
    "\n",
    "    G = get_grads(net, in_channels, input_stabilizer_size, patchnet, trainloader,\n",
    "                  kernel=(q, s),\n",
    "                  padding=(pad1, pad2),\n",
    "                  stride=(s1, s2),\n",
    "                  layer_idx=l_idx)\n",
    "    print(\"Shape after gradients: \", G.shape)\n",
    "    G = sqrt(G)\n",
    "    Gop = G.clone()\n",
    "    r_val = correlation(M, G)\n",
    "    print(\"Correlation between Trained Gcnn and AGOP: \", r_val)\n",
    "    \n",
    "    i_val = correlation(M0, G)\n",
    "    print(\"Correlation between Un-trained Gcnn and AGOP: \", i_val)\n",
    "\n",
    "    #print(\"Final: \", i_val, r_val)\n",
    "    return Gop \n",
    "    #return i_val.data.numpy(), r_val.data.numpy()\n",
    "\n",
    "\n",
    "\n",
    "def sqrt(G):\n",
    "    U, s, Vt = svd(G)\n",
    "    s = torch.pow(s, 1./2)\n",
    "    G = U @ torch.diag(s) @ Vt\n",
    "    return G\n",
    "\n",
    "\n",
    "#TODO: ADD a visualizer for the image\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b767e93-861b-4576-b360-440baf432272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_idx 2\n",
      "Computing GOP for sample 0 out of 10\n",
      "Computing GOP for sample 1 out of 10\n",
      "Computing GOP for sample 2 out of 10\n",
      "Computing GOP for sample 3 out of 10\n",
      "Computing GOP for sample 4 out of 10\n",
      "Computing GOP for sample 5 out of 10\n",
      "Computing GOP for sample 6 out of 10\n",
      "Computing GOP for sample 7 out of 10\n",
      "Computing GOP for sample 8 out of 10\n",
      "Computing GOP for sample 9 out of 10\n",
      "Computing GOP for sample 10 out of 10\n",
      "Shape after gradients:  torch.Size([360, 360])\n",
      "Correlation between Trained Gcnn and AGOP:  tensor(0.5332, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Correlation between Un-trained Gcnn and AGOP:  tensor(0.0432, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5122,  0.5156,  0.1316,  ..., -0.3701,  0.1405, -0.4765],\n",
       "        [ 0.5156,  1.7492,  0.1980,  ...,  0.2609,  0.0725, -0.2701],\n",
       "        [ 0.1316,  0.1980,  1.8413,  ..., -0.6061, -0.3391, -0.5612],\n",
       "        ...,\n",
       "        [-0.3701,  0.2609, -0.6061,  ...,  3.3044,  0.7460, -0.0309],\n",
       "        [ 0.1405,  0.0725, -0.3391,  ...,  0.7460,  2.1470,  0.8858],\n",
       "        [-0.4765, -0.2701, -0.5612,  ..., -0.0309,  0.8858,  2.7916]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_NFA(net, init_net, trainloader, layer_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e85dbcc-d04d-4a8e-a167-70a5044a8727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_idx 5\n",
      "Computing GOP for sample 0 out of 10\n",
      "Computing GOP for sample 1 out of 10\n",
      "Computing GOP for sample 2 out of 10\n",
      "Computing GOP for sample 3 out of 10\n",
      "Computing GOP for sample 4 out of 10\n",
      "Computing GOP for sample 5 out of 10\n",
      "Computing GOP for sample 6 out of 10\n",
      "Computing GOP for sample 7 out of 10\n",
      "Computing GOP for sample 8 out of 10\n",
      "Computing GOP for sample 9 out of 10\n",
      "Computing GOP for sample 10 out of 10\n",
      "Shape after gradients:  torch.Size([360, 360])\n",
      "Correlation between Trained Gcnn and AGOP:  tensor(0.5206, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Correlation between Un-trained Gcnn and AGOP:  tensor(0.0932, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4325,  0.1340, -0.2771,  ..., -0.0348, -0.1890,  0.1651],\n",
       "        [ 0.1340,  1.2878, -0.2561,  ...,  0.2383, -0.0286,  0.0565],\n",
       "        [-0.2771, -0.2561,  1.2542,  ..., -0.1439,  0.0051, -0.0365],\n",
       "        ...,\n",
       "        [-0.0348,  0.2383, -0.1439,  ...,  1.5052,  0.0078, -0.0170],\n",
       "        [-0.1890, -0.0286,  0.0051,  ...,  0.0078,  1.3672,  0.2067],\n",
       "        [ 0.1651,  0.0565, -0.0365,  ..., -0.0170,  0.2067,  1.4088]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_NFA(net, init_net, trainloader, layer_idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61bd7c7-81ca-489f-ba49-72eacf9df6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
