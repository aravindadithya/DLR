{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae23a4c-e713-4039-9d80-b967a7353fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "parent_dir='C:\\\\Users\\\\garav\\\\AGOP\\\\DLR'\n",
    "model_dir= 'C:\\\\Users\\\\garav\\\\AGOP\\\\DLR\\\\trained_models\\\\MNIST\\\\model4\\\\nn_models\\\\'\n",
    "#parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0dbcae2-852a-4c16-a540-9c9c2dd292cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from utils import agop_conv as agc\n",
    "from utils import trainer as tr\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import rfm\n",
    "import numpy as np\n",
    "from trained_models.MNIST.model4 import trainer as t\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.linalg import norm\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from utils.groupy.gconv.pytorch_gconv.splitgconv2d import P4ConvZ2, P4ConvP4\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd80ae41-ffa3-4ae8-87ab-8452edb259d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c92e10d-06bf-41fb-8580-c35bf7601fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83afe4f3-6178-42d2-a0fe-cef3228c6760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "trainloader, valloader, testloader = t.get_loaders()\n",
    "net= t.get_untrained_net()\n",
    "init_net= deepcopy(net)\n",
    "import os\n",
    "if os.path.exists(model_dir+'mnist_gcnn_trained_nn.pth'):\n",
    "    checkpoint = torch.load(model_dir+'mnist_gcnn_trained_nn.pth', map_location=torch.device(device))\n",
    "    net.load_state_dict(checkpoint['state_dict'])  # Access the 'state_dict' within the loaded dictionary\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "\n",
    "#print(\"Train the network first\")\n",
    "#t.train_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a1ba576-4192-4bd2-8d6e-dc55bfcf50d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garav\\AGOP\\DLR\\trained_models\\MNIST\\model4\\model4.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "tr.visualize_predictions(net, testloader, range(10), device, num_images=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e5bf8b-2d12-4016-9bd3-3ebd89848d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This module does the following\n",
    "1. Scan the network for conv layers\n",
    "2. For each conv layer compute W^TW of eq 3\n",
    "3. For each conv layer compute the AGOP(AJOP in case of multiple outputs)\n",
    "4. For each conv layer print the pearson correlation between 2 and 3\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "#from functorch import jacrev, vmap\n",
    "from torch.func import jacrev\n",
    "from torch.nn.functional import pad\n",
    "#import dataset\n",
    "from numpy.linalg import eig\n",
    "from copy import deepcopy\n",
    "from torch.linalg import norm, svd\n",
    "from torchvision import models\n",
    "import visdom\n",
    "from torch.linalg import norm, eig\n",
    "\n",
    "\n",
    "SEED = 2323\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "vis = visdom.Visdom('http://127.0.0.1', use_incoming_socket=False)\n",
    "vis.close(env='main')\n",
    "\n",
    "def patchify(x, patch_size, stride_size, padding=None, pad_type='zeros'):\n",
    "    '''\n",
    "        Given an input image (n,c,h,w) generate (n,w_out,h_out,c,q,s) respecting stride,padding, \n",
    "        w_out is number of pathces along the width for the given stride after padding\n",
    "        h_out is number of pathces along the height for the given stride after padding\n",
    "        (q,s) is the kernel dimensions \n",
    "    '''\n",
    "    q1, q2 = patch_size\n",
    "    s1, s2 = stride_size\n",
    "\n",
    "    if padding is None:\n",
    "        pad_1 = (q1-1)//2\n",
    "        pad_2 = (q2-1)//2\n",
    "    else:\n",
    "        pad_1, pad_2 = padding\n",
    "\n",
    "    pad_dims = (pad_2, pad_2, pad_1, pad_1)\n",
    "    if pad_type == 'zeros':\n",
    "        x = pad(x, pad_dims)\n",
    "    elif pad_type == 'circular':\n",
    "        x = pad(x, pad_dims, 'circular')\n",
    "        \n",
    "    patches = x.unfold(2, q1, s1).unfold(3, q2, s2) #(n, c, h_out, w_out, q, s)\n",
    "    patches = patches.transpose(1, 3).transpose(1, 2) #(n,w_out,h_out,c,q,s) \n",
    "    return patches\n",
    "\n",
    "class PatchConvLayer(nn.Module):\n",
    "    def __init__(self, conv_layer):\n",
    "        super().__init__()\n",
    "        self.layer = conv_layer #(k,c,q,s)\n",
    "\n",
    "    def forward(self, patches):\n",
    "        out = torch.einsum('nwhcqr, kcqr -> nwhk', patches, self.layer.weight)\n",
    "        n, w, h, k = out.shape\n",
    "        out = out.transpose(1, 3).transpose(2, 3) #(n,k,w_out,h_out)\n",
    "        return out\n",
    "\n",
    "def get_jacobian(net, data, c_idx=0, chunk=100):\n",
    "    with torch.no_grad():\n",
    "        def single_net(x):\n",
    "            # x is (w_out,h_out,c,q,s)\n",
    "            return net(x.unsqueeze(0))[:,c_idx*chunk:(c_idx+1)*chunk].squeeze(0)\n",
    "        # Parallelize across the images.\n",
    "        return torch.vmap(jacrev(single_net))(data) #(n, chunk, w_out, h_out, c, q, s)\n",
    "\n",
    "def egop(model, z):\n",
    "    ajop = 0\n",
    "    c = 10\n",
    "    chunk_idxs = 1\n",
    "    #Chunking is done to compute jacobian as chunks. This saves memory\n",
    "    #TODO: chunk should be passed as argument\n",
    "    chunk = c // chunk_idxs\n",
    "    for i in range(chunk_idxs):\n",
    "        J = get_jacobian(model, z, c_idx=i, chunk=chunk)\n",
    "        n, c, w, h, _, _, _ = J.shape\n",
    "        J = J.transpose(1, 3).transpose(1, 2) #(n, w_out, h_out, chunk, c, q, s)\n",
    "        grads = J.reshape(n*w*h, c, -1) #(n*w_out*h_out, chunk, c*q*s)\n",
    "        #Clarify: Where is mean taken\n",
    "        ajop += torch.einsum('ncd, ncD -> dD', grads, grads) #(c*q*s,c*q*s)\n",
    "    return ajop\n",
    "\n",
    "\n",
    "def load_nn(net, init_net, layer_idx=0):\n",
    "    '#TODO: Replace net with net.features'\n",
    "    \n",
    "    count = 0\n",
    "    # Get the layer_idx+1 th conv layer\n",
    "    for idx, m in enumerate(net.children()):\n",
    "        if isinstance(m, P4ConvZ2):\n",
    "            count += 1\n",
    "        if count-1 == layer_idx:\n",
    "            l_idx = idx\n",
    "            break\n",
    "\n",
    "    layers = list(net.children())\n",
    "    \n",
    "    \n",
    "    # Extract all the meta info of the current conv layer.\n",
    "    (q, s) = layer.kernel_size\n",
    "    (pad1, pad2) = layer.padding\n",
    "    (s1, s2) = layer.stride\n",
    "    # Extract W matrix\n",
    "    M = layer.weight\n",
    "    # Truncate all layers before l_idx and wrap the current conv layer as a PatchConvLayer class. \n",
    "    patchnet = deepcopy(net)\n",
    "    patchnet = layers[l_idx:]\n",
    "    layer = PatchConvLayer(layers[l_idx])\n",
    "    patchnet[0] = layer\n",
    "\n",
    "    count = -1\n",
    "    for idx, p in enumerate(net.parameters()):\n",
    "        \n",
    "        # This logic of identifying layer_idx+1th parameters is not generic. It will fail if Batchnorm2d is present\n",
    "        # Why not get weights directly from the layer object?\n",
    "        if len(p.shape) > 1:\n",
    "            count += 1\n",
    "        if count == layer_idx:\n",
    "            M = p.data #(k,c,q,s)\n",
    "            _, ki, q, s = M.shape\n",
    "            \n",
    "            # Build W which is a (k, c*q*s) matrix\n",
    "            M = M.reshape(-1, ki*q*s)\n",
    "            \n",
    "            # Compute WtW which is (c*q*s,c*q*s) matrix\n",
    "            M = torch.einsum('nd, nD -> dD', M, M)\n",
    "            \n",
    "            # Build W0 from the untrained net which is a (k, c*q*s) matrix\n",
    "            M0 = [p for p in init_net.parameters()][idx]          \n",
    "            M0 = M0.reshape(-1, ki*q*s)\n",
    "            \n",
    "            # Compute W0tW0 which is (c*q*s,c*q*s) matrix\n",
    "            M0 = torch.einsum('nd, nD -> dD', M0, M0)\n",
    "            break\n",
    "\n",
    "    return net, patchnet, M, M0, l_idx, [(q, s), (pad1,pad2), (s1,s2)]\n",
    "\n",
    "\n",
    "def get_grads(net, patchnet, trainloader,\n",
    "              kernel=(3,3), padding=(1,1),\n",
    "              stride=(1,1), layer_idx=0):\n",
    "    net.eval()\n",
    "    net.cuda()\n",
    "    patchnet.eval()\n",
    "    patchnet.cuda()\n",
    "    M = 0\n",
    "    q, s = kernel\n",
    "    pad1, pad2 = padding\n",
    "    s1, s2 = stride\n",
    "\n",
    "    # Num images for taking AGOP (Can be small for early layers)\n",
    "    MAX_NUM_IMGS = 10\n",
    "\n",
    "    for idx, batch in enumerate(trainloader):\n",
    "        print(\"Computing GOP for sample \" + str(idx) + \\\n",
    "              \" out of \" + str(MAX_NUM_IMGS))\n",
    "        imgs, _ = batch\n",
    "        with torch.no_grad():\n",
    "            imgs = imgs.cuda()        \n",
    "            # Run the first half of the network wrt to the current layer \n",
    "            imgs = net.features[:layer_idx](imgs).cpu() #(n,c,h,w)\n",
    "        patches = patchify(imgs, (q, s), (s1,s2), padding=(pad1,pad2))#(n,w_out,h_out,c,q,s)\n",
    "        patches = patches.cuda()\n",
    "        #print(patches.shape)\n",
    "        M += egop(patchnet, patches).cpu()\n",
    "        del imgs, patches\n",
    "        torch.cuda.empty_cache()\n",
    "        if idx >= MAX_NUM_IMGS:\n",
    "            break\n",
    "    net.cpu()\n",
    "    patchnet.cpu()\n",
    "    return M\n",
    "\n",
    "\n",
    "def min_max(M):\n",
    "    return (M - M.min()) / (M.max() - M.min())\n",
    "\n",
    "\n",
    "def correlation(M1, M2):\n",
    "    M1 -= M1.mean()\n",
    "    M2 -= M2.mean()\n",
    "\n",
    "    norm1 = norm(M1.flatten())\n",
    "    norm2 = norm(M2.flatten())\n",
    "\n",
    "    return torch.sum(M1.cuda() * M2.cuda()) / (norm1 * norm2)\n",
    "\n",
    "\n",
    "def verify_NFA(net, init_net, trainloader, layer_idx=0):\n",
    "\n",
    "\n",
    "    net, patchnet, M, M0, l_idx, conv_vals = load_nn(net,\n",
    "                                                     init_net,\n",
    "                                                     layer_idx=layer_idx)\n",
    "    (q, s), (pad1, pad2), (s1, s2) = conv_vals\n",
    "\n",
    "    i_val = correlation(M0, M)\n",
    "    print(\"Correlation between Initial and Trained CNFM: \", i_val)\n",
    "\n",
    "    G = get_grads(net, patchnet, trainloader,\n",
    "                  kernel=(q, s),\n",
    "                  padding=(pad1, pad2),\n",
    "                  stride=(s1, s2),\n",
    "                  layer_idx=l_idx)\n",
    "    print(\"Shpae after gradients: \", G.shape)\n",
    "    G = sqrt(G)\n",
    "    Gop = G.clone()\n",
    "    r_val = correlation(M, G)\n",
    "    print(\"Correlation between Trained CNFM and AGOP: \", r_val)\n",
    "    print(\"Final: \", i_val, r_val)\n",
    "    return Gop \n",
    "    #return i_val.data.numpy(), r_val.data.numpy()\n",
    "\n",
    "\n",
    "\n",
    "def sqrt(G):\n",
    "    U, s, Vt = svd(G)\n",
    "    s = torch.pow(s, 1./2)\n",
    "    G = U @ torch.diag(s) @ Vt\n",
    "    return G\n",
    "\n",
    "'''\n",
    "def main():\n",
    "\n",
    "    # Adjust to index conv layers in VGGs\n",
    "    idxs = list(range(8))\n",
    "\n",
    "    fname = 'csv_logs/test.csv'\n",
    "    outf = open(fname, 'w')\n",
    "\n",
    "    net = models.vgg11(weights=\"DEFAULT\")\n",
    "    #init_net is used as a reference untrained network.\n",
    "    init_net = models.vgg11(weights=None)\n",
    "\n",
    "    # Modules is unused.\n",
    "    modules= list(net.children())[:-1]\n",
    "    modules += [nn.Flatten(), list(net.children())[-1]]\n",
    "\n",
    "    # Set path to imagenet data\n",
    "    path = None\n",
    "\n",
    "    trainloader, _ = dataset.get_imagenet(batch_size=2, path=path)\n",
    "\n",
    "    for idx in idxs:\n",
    "        i_val, r_val = verify_NFA(net, init_net, trainloader, layer_idx=idx)\n",
    "        print(\"Layer \" + str(idx+1) + ',' + str(i_val) + ',' + str(r_val), file=outf, flush=True)\n",
    "'''\n",
    "#TODO: ADD a visualizer for the image\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1016c9c-9f91-4650-8e54-c3013b26ff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "0\n",
      "Parameter containing:\n",
      "tensor([[[[[ 0.0292,  0.2141,  0.0165],\n",
      "           [ 0.4765,  0.1134, -0.4281],\n",
      "           [ 0.2660, -0.1225,  0.1569]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.3971,  0.3040, -0.0782],\n",
      "           [-0.2174,  0.3545, -0.1848],\n",
      "           [ 0.1032,  0.2699,  0.3735]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.1157,  0.3254, -0.1751],\n",
      "           [-0.1815, -0.2930, -0.2022],\n",
      "           [ 0.1058, -0.1116, -0.3284]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.2150, -0.0393, -0.3234],\n",
      "           [ 0.4192, -0.2133, -0.1919],\n",
      "           [ 0.4808,  0.4288, -0.2992]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.0141,  0.2815, -0.0548],\n",
      "           [ 0.2867, -0.1359, -0.3368],\n",
      "           [ 0.0087, -0.2200,  0.1808]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.4491, -0.2679, -0.2949],\n",
      "           [ 0.0531,  0.1287, -0.3561],\n",
      "           [ 0.2307, -0.2356, -0.3471]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.0144,  0.2857,  0.1793],\n",
      "           [ 0.2572,  0.3503, -0.1913],\n",
      "           [ 0.1426, -0.2798, -0.1957]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.3386,  0.1300, -0.1025],\n",
      "           [ 0.1340,  0.2924, -0.4710],\n",
      "           [ 0.5221,  0.1273, -0.2904]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.0140, -0.2038, -0.2024],\n",
      "           [ 0.0614,  0.1979, -0.1601],\n",
      "           [-0.3146,  0.0530, -0.0572]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.3300,  0.1862, -0.0882],\n",
      "           [ 0.0076,  0.5200,  0.1462],\n",
      "           [ 0.0123,  0.3221,  0.1396]]]]], requires_grad=True)\n",
      "(3, 3)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "[PatchConvLayer(\n",
      "  (layer): P4ConvZ2()\n",
      "), P4ConvP4(), P4ConvP4(), P4ConvP4(), Linear(in_features=1280, out_features=50, bias=True), Linear(in_features=50, out_features=10, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "class PatchConvLayer(nn.Module):\n",
    "    def __init__(self, conv_layer):\n",
    "        super().__init__()\n",
    "        self.layer = conv_layer #(k,c,q,s)\n",
    "\n",
    "    def forward(self, patches):\n",
    "        out = torch.einsum('nwhcqr, kcqr -> nwhk', patches, self.layer.weight)\n",
    "        n, w, h, k = out.shape\n",
    "        out = out.transpose(1, 3).transpose(2, 3) #(n,k,w_out,h_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "layer_idx=0\n",
    "count=0\n",
    "# Get the layer_idx+1 th conv layer\n",
    "for idx, m in enumerate(net.children()):\n",
    "        if isinstance(m, P4ConvZ2):\n",
    "            print(\"hi\")\n",
    "            count += 1\n",
    "        if count-1 == layer_idx:\n",
    "            l_idx = idx\n",
    "            break\n",
    "layers= list(net.children())\n",
    "\n",
    "layer = layers[l_idx]\n",
    "print(l_idx)\n",
    "print(layer.weight)\n",
    "print(layer.kernel_size)\n",
    "print(layer.padding)\n",
    "print(layer.stride)\n",
    "patchnet = deepcopy(net)\n",
    "layer = PatchConvLayer(layers[l_idx])\n",
    "patchnet= layers[l_idx:]\n",
    "patchnet[0] = layer\n",
    "print(patchnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b23ff-d3b5-4636-8e84-13f276a135ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
